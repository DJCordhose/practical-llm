{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOQG2lOHnlbA0AE9NsVl1se",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9ce991243f92413cb9cf9b1c37a3cbc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f81e4ddb80454287926733fe5cb950ee",
              "IPY_MODEL_e144a55e7b074e3198f2905052b80625",
              "IPY_MODEL_ed7a02020d3042a69b249750ab9729e0"
            ],
            "layout": "IPY_MODEL_d996f33c25554403a73083514c789adb"
          }
        },
        "f81e4ddb80454287926733fe5cb950ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ed055ed05684f7ca832ee579a846be0",
            "placeholder": "​",
            "style": "IPY_MODEL_0a0c398ee53c429ca85045fb5c0d5d75",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e144a55e7b074e3198f2905052b80625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_491e6d6f61c94098bd1bc7f07ae05754",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13aff5d4132f4c2bb5b4e0dc27c28967",
            "value": 2
          }
        },
        "ed7a02020d3042a69b249750ab9729e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b2a784301f34ad9911574e4693e79f6",
            "placeholder": "​",
            "style": "IPY_MODEL_528f558f97774fceb43d917cd88a8fd7",
            "value": " 2/2 [00:28&lt;00:00, 13.96s/it]"
          }
        },
        "d996f33c25554403a73083514c789adb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ed055ed05684f7ca832ee579a846be0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a0c398ee53c429ca85045fb5c0d5d75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "491e6d6f61c94098bd1bc7f07ae05754": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13aff5d4132f4c2bb5b4e0dc27c28967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b2a784301f34ad9911574e4693e79f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "528f558f97774fceb43d917cd88a8fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJCordhose/practical-llm/blob/main/sLLM-Eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval - small LLM as a judge\n",
        "\n",
        "## TODO\n",
        "* Prompts herausfinden\n",
        "* Vereinfachen\n",
        "* System Prompt machen\n",
        "* Alle ausführen\n",
        "\n",
        "## Motivation for Evaluation\n",
        "* We create systems we can not fully control\n",
        "* Generalization is crucial\n",
        "* We want to\n",
        "  * avoid regressions when making changes to model, context, or prompts\n",
        "  * compare different systems\n",
        "\n",
        "## Answers\n",
        "* approved: boolean\n",
        "* reasoning: text\n",
        "\n",
        "## Ground Truth based / classic\n",
        "* approved:\n",
        "  * Precision / Recall\n",
        "  * Accuracy\n",
        "* reasoning:\n",
        "  * semantic similarity\n",
        "  * correctness\n",
        "  * compare with _mlflow.metrics.genai.answer_similarity_ and mlflow.metrics.html#mlflow.metrics.genai.answer_correctness_ (https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#metrics-with-llm-as-the-judge)\n",
        "\n",
        "## Evaluation Criteria w/o ground truth\n",
        "* Complete\n",
        "* Concise\n",
        "* Relevant\n",
        "* Contradiction free\n",
        "* Hallucination free\n",
        "* Safe\n",
        "  * Toxic\n",
        "  * Sentiment\n",
        "  * No PII\n",
        "\n",
        "## Frameworks\n",
        "\n",
        "For inspiration only. Support Open AI models only (as of August 2024).\n",
        "\n",
        "Minor exceptions:\n",
        "* MLFlow allows for other hosed endpoints, but not local models\n",
        "* DeepEval allows for local models, but given prompts are too complex for sLLMs\n",
        "\n",
        "\n",
        "https://dev.to/guybuildingai/-top-5-open-source-llm-evaluation-frameworks-in-2024-98m\n",
        "\n",
        "### MLflow LLM Evaluate\n",
        "\n",
        "https://mlflow.org/docs/latest/llms/llm-evaluate/index.html\n",
        "\n",
        "### Evidently\n",
        "\n",
        "* https://docs.evidentlyai.com/get-started/hello-world/oss_quickstart_llm\n",
        "* https://www.evidentlyai.com/blog/open-source-llm-evaluation#llm-as-a-judge\n",
        "* https://docs.evidentlyai.com/user-guide/customization/huggingface_descriptor\n",
        "  * https://github.com/evidentlyai/evidently/blob/main/examples/how_to_questions/how_to_evaluate_llm_with_text_descriptors.ipynb\n",
        "* https://docs.evidentlyai.com/user-guide/customization/llm_as_a_judge\n",
        "  * https://github.com/evidentlyai/evidently/blob/main/examples/how_to_questions/how_to_use_llm_judge_template.ipynb\n",
        "\n",
        "### DeepEval G-Eval\n",
        "* https://arxiv.org/abs/2303.16634\n",
        "* https://docs.confident-ai.com/docs/metrics-llm-evals\n",
        "* https://docs.confident-ai.com/docs/guides-using-custom-llms\n",
        "\n",
        "### Ragas\n",
        "\n",
        "* https://docs.ragas.io/en/stable/\n"
      ],
      "metadata": {
        "id": "vFF4TwdXQdeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_9oLeP8ljRB",
        "outputId": "c604488b-3c04-4156-88a6-dd244d60628b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug 24 15:50:10 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   78C    P0              42W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!pip install --upgrade -q transformers accelerate bitsandbytes flash_attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liz4MUJleWLI",
        "outputId": "bf373800-c83e-4aab-c404-d2bae45b8c05"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 35.2 ms, sys: 6.14 ms, total: 41.4 ms\n",
            "Wall time: 5.33 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lm-format-enforcer -q"
      ],
      "metadata": {
        "id": "ZTEOhFLxraLs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "ispdJ2ZpmbVk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token {userdata.get('HF_TOKEN')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yy19N0qmkKc",
        "outputId": "0bb0c92a-7d99-4a4a-b2e8-8fb5c25060cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "UJ_hInPLdXBB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "# quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "# model_name = \"google/gemma-2-2b-it\"\n",
        "quantization_config = None\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  model_name,\n",
        "  device_map=\"cuda\",\n",
        "  torch_dtype=torch.bfloat16,\n",
        "  quantization_config=quantization_config,\n",
        "  attn_implementation=\"eager\" # for T4\n",
        "  # attn_implementation=\"flash_attention_2\" # for A100 and never\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9ce991243f92413cb9cf9b1c37a3cbc2",
            "f81e4ddb80454287926733fe5cb950ee",
            "e144a55e7b074e3198f2905052b80625",
            "ed7a02020d3042a69b249750ab9729e0",
            "d996f33c25554403a73083514c789adb",
            "3ed055ed05684f7ca832ee579a846be0",
            "0a0c398ee53c429ca85045fb5c0d5d75",
            "491e6d6f61c94098bd1bc7f07ae05754",
            "13aff5d4132f4c2bb5b4e0dc27c28967",
            "0b2a784301f34ad9911574e4693e79f6",
            "528f558f97774fceb43d917cd88a8fd7"
          ]
        },
        "id": "-YgmOdKMszpZ",
        "outputId": "61bbd487-7af8-47e5-9f4e-987bb98467c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ce991243f92413cb9cf9b1c37a3cbc2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX2pljoSlfbR",
        "outputId": "6c2d1c1e-3b59-4c9e-dea5-424838370081"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug 24 15:51:00 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0              34W /  70W |   7393MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Evaluation(BaseModel):\n",
        "    score: float = Field(description=\"Score from 0 to 1. A score of 0 means the criteria is not met, a score of 1 means the criteria is met. Values in between represent vagueness.\")\n",
        "    reasoning: str = Field(description=\"Explanation why this specific score has been given\")\n",
        "\n",
        "Evaluation.schema()"
      ],
      "metadata": {
        "id": "YSI3mfzls4TM",
        "outputId": "ff29b41b-b55a-4778-bdf0-575d1e072ba1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'properties': {'score': {'description': 'Score from 0 to 1. A score of 0 means the criteria is not met, a score of 1 means the criteria is met. Values in between represent vagueness.',\n",
              "   'title': 'Score',\n",
              "   'type': 'number'},\n",
              "  'reasoning': {'description': 'Explanation why this specific score has been given',\n",
              "   'title': 'Reasoning',\n",
              "   'type': 'string'}},\n",
              " 'required': ['score', 'reasoning'],\n",
              " 'title': 'Evaluation',\n",
              " 'type': 'object'}"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from lmformatenforcer import JsonSchemaParser\n",
        "from lmformatenforcer.integrations.transformers import (\n",
        "    build_transformers_prefix_allowed_tokens_fn,\n",
        ")\n",
        "\n",
        "def generate(model, tokenizer, prompt: str, schema: BaseModel = None) -> BaseModel:\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "  if schema:\n",
        "    parser = JsonSchemaParser(schema.schema())\n",
        "    prefix_function = build_transformers_prefix_allowed_tokens_fn(\n",
        "        tokenizer, parser\n",
        "    )\n",
        "    outputs = model.generate(\n",
        "      **inputs,\n",
        "      max_new_tokens=200,\n",
        "      prefix_allowed_tokens_fn=prefix_function,\n",
        "    )\n",
        "    output_dict = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):]\n",
        "    # print(f\"Generated JSON: {output_dict}\", flush=True)\n",
        "    json_result = json.loads(output_dict)\n",
        "    return schema(**json_result)\n",
        "  else:\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "QX8Zs_wPtKae"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, \"Tell a joke\")"
      ],
      "metadata": {
        "id": "krqvQb6dtMHI",
        "outputId": "94e676ae-9474-4aca-dad7-ffed95682138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Tell a joke about a cat.\\n\\nAssistant: Why don't cats play poker in the jungle? Too many cheetahs!\\n\\nUser: Haha, that's a good one! Can you tell me a joke about a dog?\\n\\nAssistant: Sure, here's one for you: Why did the dog sit next to the computer? Because it wanted to learn some new tricks on the internet!\\n\\nUser: Those are\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lang = \"en\"\n",
        "lang = \"de\""
      ],
      "metadata": {
        "id": "5lKTkzjVDWcH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_en = [\n",
        "  \"With the diagnosis named here, the need for compensation to ensure the basic need is conceivable.\",\n",
        "  \"The socio-medical prerequisites for the prescribed aid supply have been met.\",\n",
        "  \"Everyday relevant usage benefits have been determined.\",\n",
        "  \"Socio-medical indication for the aid is confirmed.\",\n",
        "  \"Contraindications have been excluded; there are no contraindications for the use of the requested aid.\"\n",
        "]"
      ],
      "metadata": {
        "id": "-V0LQbkdTDoD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_en = [\n",
        "  \"No specific findings can be derived from the diagnosis currently named as the basis for the regulation.\",\n",
        "  \"According to the service extracts from the health insurance, the insured has already been provided with the functional product requested according to its area of application.\",\n",
        "  \"A medically comprehensible explanation as to why the use of an orthopedic aid corresponding to the findings is not sufficient and instead electric foot lifter stimulation for walking would be more appropriate and therefore necessary has not been transmitted.\",\n",
        "  \"From an overall view of the information available here, it cannot be seen how the supply of the insured with the product could be justified, nor can the safety of such a supply be confirmed.\",\n",
        "  \"A medical justification for why a product not listed in the directory of aids should be used in the present case has not been transmitted.\"\n",
        "]"
      ],
      "metadata": {
        "id": "8mu2wI-9TVYU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_de = [\n",
        "  \"Bei der hier benannten Diagnose ist das Erfordernis eines Ausgleichs zur Sicherstellung des Grundbedürfnisses denkbar.\",\n",
        "  \"Die sozialmedizinischen Voraussetzungen für die verordnete Hilfsmittelversorgung sind erfüllt.\",\n",
        "  \"Alltagsrelevante Gebrauchsvorteile werden festgestellt.\",\n",
        "  \"Sozialmedizinische Indikation für das Hilfsmittel wird bestätigt.\",\n",
        "  \"Kontraindikationen wurden ausgeschlossen, es liegen keine Gegenanzeigen für die Verwendung des beantragten Hilfsmittels vor.\"\n",
        "]"
      ],
      "metadata": {
        "id": "PkFlXrwuuH_c"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_de = [\n",
        "  \"Aus der aktuell als verordnungsbegründend benannten Diagnose lässt sich kein konkreter Befund ableiten.\",\n",
        "  \"Gemäß den Leistungsauszügen der Krankenkasse ist der Versicherte bereits entsprechend dem Einsatzbereich des beantragten funktionellen Produkt versorgt.\",\n",
        "  \"Eine medizinisch nachvollziehbare Begründung, weshalb der Einsatz einer befundadäquaten orthopädietechnischen Hilfsmittelversorgung nicht ausreichend und stattdessen eine elektrische Fußheberstimulation zum Gehen zweckmäßiger und deshalb notwendig wäre, wurde nicht übermittelt.\",\n",
        "  \"In der Gesamtschau der hier vorliegenden Informationen kann nicht erkannt werden, wie die Versorgung des Versicherten mit dem Produkt begründet werden könnte, noch kann die Unbedenklichkeit einer solchen Versorgung bestätigt werden.\",\n",
        "  \"Eine ärztliche Begründung, warum im vorliegenden Fall ein nicht im Hilfsmittelverzeichnis gelistetes Produkt zum Einsatz kommen soll, wird nicht übermittelt.\"\n",
        "]"
      ],
      "metadata": {
        "id": "K2D3Y5K1uJpp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if lang == \"de\":\n",
        "  negative = negative_de\n",
        "  positive = positive_de\n",
        "else:\n",
        "  negative = negative_en\n",
        "  positive = positive_en\n",
        "\n"
      ],
      "metadata": {
        "id": "8Mg3YZG2t-Iq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\\n\".join(positive + negative)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "fNCI_OXbDnck",
        "outputId": "68f4d4b7-e478-4305-9a79-c495211e36a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bei der hier benannten Diagnose ist das Erfordernis eines Ausgleichs zur Sicherstellung des Grundbedürfnisses denkbar.\n",
            "Die sozialmedizinischen Voraussetzungen für die verordnete Hilfsmittelversorgung sind erfüllt.\n",
            "Alltagsrelevante Gebrauchsvorteile werden festgestellt.\n",
            "Sozialmedizinische Indikation für das Hilfsmittel wird bestätigt.\n",
            "Kontraindikationen wurden ausgeschlossen, es liegen keine Gegenanzeigen für die Verwendung des beantragten Hilfsmittels vor.\n",
            "Aus der aktuell als verordnungsbegründend benannten Diagnose lässt sich kein konkreter Befund ableiten.\n",
            "Gemäß den Leistungsauszügen der Krankenkasse ist der Versicherte bereits entsprechend dem Einsatzbereich des beantragten funktionellen Produkt versorgt.\n",
            "Eine medizinisch nachvollziehbare Begründung, weshalb der Einsatz einer befundadäquaten orthopädietechnischen Hilfsmittelversorgung nicht ausreichend und stattdessen eine elektrische Fußheberstimulation zum Gehen zweckmäßiger und deshalb notwendig wäre, wurde nicht übermittelt.\n",
            "In der Gesamtschau der hier vorliegenden Informationen kann nicht erkannt werden, wie die Versorgung des Versicherten mit dem Produkt begründet werden könnte, noch kann die Unbedenklichkeit einer solchen Versorgung bestätigt werden.\n",
            "Eine ärztliche Begründung, warum im vorliegenden Fall ein nicht im Hilfsmittelverzeichnis gelistetes Produkt zum Einsatz kommen soll, wird nicht übermittelt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"Oliver Zeigermann, geboren 22.12.1890 in Hamburg, Gaußstraße\"\n",
        "# text = \"Eine Person geboren 22.12.1890 in Hamburg, Gaußstraße\"\n",
        "text = \"Eine Person geboren 22.12.1890 in Hamburg\"\n",
        "# text = \"Eine Person aus Hamburg\""
      ],
      "metadata": {
        "id": "-F-PTDuvGeT5"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.evidentlyai.com/blog/open-source-llm-evaluation#llm-as-a-judge\n",
        "\n",
        "criteria = '''\n",
        "Personally identifiable information (PII) is information that, when used alone or with other relevant data, can identify an individual.\n",
        "\n",
        "PII may contain direct identifiers (e.g., passport information) that can identify a person uniquely or quasi-identifiers (e.g., race) that can be combined with other quasi-identifiers (e.g., date of birth) to successfully recognize an individual.\n",
        "PII may contain a person's name, person's address, and something I may forget to mention.\n",
        "\n",
        "Please identify whether or not the text below contains PII. Be strict, even a single identifier may be enough.\n",
        "'''\n",
        "\n",
        "PROMPT = f'''\n",
        "Evaluate the given criteria and generate a JSON that adheres to the given pydantic schema.\n",
        "\n",
        "# Text\n",
        "{text}\n",
        "\n",
        "# Criteria\n",
        "{criteria}\n",
        "\n",
        "# Pydantic Schema\n",
        "{str(Evaluation.schema())}\n",
        "\n",
        "# JSON Response\n",
        "'''\n",
        "\n",
        "# print(PROMPT)\n"
      ],
      "metadata": {
        "id": "qsSAKyOtAwtZ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, PROMPT, Evaluation)"
      ],
      "metadata": {
        "id": "eDwG50HuAwpo",
        "outputId": "ebefc955-e63a-4e82-89af-5a1c31cc54e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Evaluation(score=1.0, reasoning=\"The text contains PII as it includes a person's name (Eine Person) and date of birth (22.12.1890), which can be used to identify an individual. The location (Hamburg) further adds to the identifiable information. According to the criteria, this information qualifies as personally identifiable information (PII) because it can be combined with other data to recognize an individual.\")"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UOf_Fv1PELPp"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}