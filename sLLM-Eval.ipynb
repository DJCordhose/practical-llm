{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOy4ObVGNuW3Tl4Aq1EbQFZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a688c6c1c03e459fa40e14863406322c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f4fd288ec394464878d5329371df927",
              "IPY_MODEL_bbbed3669bf14b6889cc57012373d1bf",
              "IPY_MODEL_154e0386216e4852aebf375ce8e8890c"
            ],
            "layout": "IPY_MODEL_f3feaf35a8b043708dd16b2a1415d6a7"
          }
        },
        "7f4fd288ec394464878d5329371df927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90025908556e4c9c8de7f568315ad2c4",
            "placeholder": "​",
            "style": "IPY_MODEL_78da198766064a799240328d183a3c05",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "bbbed3669bf14b6889cc57012373d1bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4bd92a21ee94bb4b2148ee0aad57990",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1dcf336cf78a40c6b59c2a699ca94dcd",
            "value": 2
          }
        },
        "154e0386216e4852aebf375ce8e8890c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e7edcf97408459f9a4c8733b39f9820",
            "placeholder": "​",
            "style": "IPY_MODEL_ecde2581d25d4ef88b96138e95a7d04e",
            "value": " 2/2 [00:30&lt;00:00, 14.58s/it]"
          }
        },
        "f3feaf35a8b043708dd16b2a1415d6a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90025908556e4c9c8de7f568315ad2c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78da198766064a799240328d183a3c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4bd92a21ee94bb4b2148ee0aad57990": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dcf336cf78a40c6b59c2a699ca94dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e7edcf97408459f9a4c8733b39f9820": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecde2581d25d4ef88b96138e95a7d04e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJCordhose/practical-llm/blob/main/sLLM-Eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval - small LLM as a judge\n",
        "\n",
        "## Motivation for Evaluation\n",
        "* We create systems we can not fully control\n",
        "* Generalization is crucial\n",
        "* We want to\n",
        "  * avoid regressions when making changes to model, context, or prompts\n",
        "  * compare different systems\n",
        "\n",
        "\n",
        "### Regressions in Versions\n",
        "![Regressions in Versions](https://raw.githubusercontent.com/DJCordhose/practical-llm/main/llm_regression.jpg \"Regressions in Versions\")\n",
        "\n",
        "\n",
        "## Arguments for evaluation\n",
        "* (retrieval) context: the individual assessment\n",
        "* input (fixed question, defined by static prompt): What is the result of the assessment? ...\n",
        "* actual output: Yes/No, explanation\n",
        "* expected output: curated GT explanation\n",
        "\n",
        "\n",
        "## Answers\n",
        "* approved: boolean\n",
        "* reasoning: text\n",
        "\n",
        "\n",
        "## Ground Truth based / classic\n",
        "* approved:\n",
        "  * Precision / Recall\n",
        "  * Accuracy\n",
        "* reasoning:\n",
        "  * semantic similarity\n",
        "  * correctness\n",
        "  * compare with _mlflow.metrics.genai.answer_similarity_ and mlflow.metrics.html#mlflow.metrics.genai.answer_correctness_ (https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#metrics-with-llm-as-the-judge)\n",
        "\n",
        "\n",
        "## Evaluation Criteria w/o ground truth\n",
        "* Complete\n",
        "* Concise\n",
        "* Relevant\n",
        "* Contradiction free\n",
        "* Hallucination free\n",
        "* Form\n",
        "  * Formal? Casual?\n",
        "  * Grammar / Spelling\n",
        "  * Style of writing\n",
        "* Safe\n",
        "  * Toxic\n",
        "  * Sentiment\n",
        "  * No PII\n",
        "\n",
        "\n",
        "## Frameworks\n",
        "\n",
        "For inspiration only. Support Open AI models only (as of August 2024). Good starting point for an overview: https://docs.confident-ai.com/docs/metrics-introduction\n",
        "\n",
        "Minor exceptions:\n",
        "* MLFlow allows for other hosed endpoints, but not local models\n",
        "* DeepEval allows for local models, but given prompts are too complex for sLLMs\n",
        "\n",
        "\n",
        "https://dev.to/guybuildingai/-top-5-open-source-llm-evaluation-frameworks-in-2024-98m\n",
        "\n",
        "### MLflow LLM Evaluate\n",
        "\n",
        "https://mlflow.org/docs/latest/llms/llm-evaluate/index.html\n",
        "\n",
        "### Evidently\n",
        "\n",
        "* https://docs.evidentlyai.com/get-started/hello-world/oss_quickstart_llm\n",
        "* https://www.evidentlyai.com/blog/open-source-llm-evaluation#llm-as-a-judge\n",
        "* https://docs.evidentlyai.com/user-guide/customization/huggingface_descriptor\n",
        "  * https://github.com/evidentlyai/evidently/blob/main/examples/how_to_questions/how_to_evaluate_llm_with_text_descriptors.ipynb\n",
        "* https://docs.evidentlyai.com/user-guide/customization/llm_as_a_judge\n",
        "  * https://github.com/evidentlyai/evidently/blob/main/examples/how_to_questions/how_to_use_llm_judge_template.ipynb\n",
        "\n",
        "### DeepEval G-Eval\n",
        "* https://arxiv.org/abs/2303.16634\n",
        "* https://docs.confident-ai.com/docs/metrics-llm-evals\n",
        "* https://docs.confident-ai.com/docs/guides-using-custom-llms\n",
        "\n",
        "### Ragas\n",
        "\n",
        "* https://docs.ragas.io/en/stable/\n"
      ],
      "metadata": {
        "id": "vFF4TwdXQdeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_9oLeP8ljRB",
        "outputId": "8e18de8d-834e-4afe-afd2-cc8db9bdd789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug 24 16:12:23 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0              34W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!pip install --upgrade -q transformers accelerate bitsandbytes flash_attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liz4MUJleWLI",
        "outputId": "c2ed930e-142c-4a77-953b-6970a7e98ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 44.2 ms, sys: 8.83 ms, total: 53 ms\n",
            "Wall time: 6.54 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lm-format-enforcer -q"
      ],
      "metadata": {
        "id": "ZTEOhFLxraLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "ispdJ2ZpmbVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token {userdata.get('HF_TOKEN')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yy19N0qmkKc",
        "outputId": "ad226f3c-e570-4378-8fa8-72042f2eecf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "UJ_hInPLdXBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "# quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "# model_name = \"google/gemma-2-2b-it\"\n",
        "quantization_config = None\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  model_name,\n",
        "  device_map=\"cuda\",\n",
        "  torch_dtype=torch.bfloat16,\n",
        "  quantization_config=quantization_config,\n",
        "  attn_implementation=\"eager\" # for T4\n",
        "  # attn_implementation=\"flash_attention_2\" # for A100 and never\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a688c6c1c03e459fa40e14863406322c",
            "7f4fd288ec394464878d5329371df927",
            "bbbed3669bf14b6889cc57012373d1bf",
            "154e0386216e4852aebf375ce8e8890c",
            "f3feaf35a8b043708dd16b2a1415d6a7",
            "90025908556e4c9c8de7f568315ad2c4",
            "78da198766064a799240328d183a3c05",
            "c4bd92a21ee94bb4b2148ee0aad57990",
            "1dcf336cf78a40c6b59c2a699ca94dcd",
            "2e7edcf97408459f9a4c8733b39f9820",
            "ecde2581d25d4ef88b96138e95a7d04e"
          ]
        },
        "id": "-YgmOdKMszpZ",
        "outputId": "56a54cd2-3ea8-4fd3-d99d-b207e3b88ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a688c6c1c03e459fa40e14863406322c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX2pljoSlfbR",
        "outputId": "2e6f6e5f-ba02-4cd9-ea68-58712ae5a5c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug 24 16:13:15 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P0              34W /  70W |   7393MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Evaluation(BaseModel):\n",
        "    score: float = Field(description=\"Score from 0 to 1. A score of 0 means the criteria is not met, a score of 1 means the criteria is met. Values in between represent vagueness.\")\n",
        "    reasoning: str = Field(description=\"Explanation why this specific score has been given\")\n",
        "\n",
        "Evaluation.schema()"
      ],
      "metadata": {
        "id": "YSI3mfzls4TM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "324e3132-3096-47c1-cc45-54113ab0a03d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'properties': {'score': {'description': 'Score from 0 to 1. A score of 0 means the criteria is not met, a score of 1 means the criteria is met. Values in between represent vagueness.',\n",
              "   'title': 'Score',\n",
              "   'type': 'number'},\n",
              "  'reasoning': {'description': 'Explanation why this specific score has been given',\n",
              "   'title': 'Reasoning',\n",
              "   'type': 'string'}},\n",
              " 'required': ['score', 'reasoning'],\n",
              " 'title': 'Evaluation',\n",
              " 'type': 'object'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from lmformatenforcer import JsonSchemaParser\n",
        "from lmformatenforcer.integrations.transformers import (\n",
        "    build_transformers_prefix_allowed_tokens_fn,\n",
        ")\n",
        "\n",
        "def generate(model, tokenizer, prompt: str, schema: BaseModel = None) -> BaseModel:\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "  if schema:\n",
        "    parser = JsonSchemaParser(schema.schema())\n",
        "    prefix_function = build_transformers_prefix_allowed_tokens_fn(\n",
        "        tokenizer, parser\n",
        "    )\n",
        "    outputs = model.generate(\n",
        "      **inputs,\n",
        "      max_new_tokens=200,\n",
        "      prefix_allowed_tokens_fn=prefix_function,\n",
        "    )\n",
        "    output_dict = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):]\n",
        "    # print(f\"Generated JSON: {output_dict}\", flush=True)\n",
        "    json_result = json.loads(output_dict)\n",
        "    return schema(**json_result)\n",
        "  else:\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "QX8Zs_wPtKae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, \"Tell a joke\")"
      ],
      "metadata": {
        "id": "krqvQb6dtMHI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5f483338-25d5-4219-cfcc-417a2d4dd960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Tell a joke about a cat.\\n\\nAssistant: Why don't cats play poker in the jungle? Too many cheetahs!\\n\\nUser: Haha, that's a good one! Can you tell me a joke about a dog?\\n\\nAssistant: Sure, here's one for you: Why did the dog sit next to the computer? Because it wanted to learn some new tricks on the internet!\\n\\nUser: Those are\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "CIm5MN0Hjk7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ground truth"
      ],
      "metadata": {
        "id": "Mr4-YXakkm1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assessments as Contexts"
      ],
      "metadata": {
        "id": "qvmD8vmmjd_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lang = \"en\"\n",
        "lang = \"de\""
      ],
      "metadata": {
        "id": "5lKTkzjVDWcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_en = [\n",
        "  \"With the diagnosis named here, the need for compensation to ensure the basic need is conceivable.\",\n",
        "  \"The socio-medical prerequisites for the prescribed aid supply have been met.\",\n",
        "  \"Everyday relevant usage benefits have been determined.\",\n",
        "  \"Socio-medical indication for the aid is confirmed.\",\n",
        "  \"Contraindications have been excluded; there are no contraindications for the use of the requested aid.\"\n",
        "]"
      ],
      "metadata": {
        "id": "-V0LQbkdTDoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_en = [\n",
        "  \"No specific findings can be derived from the diagnosis currently named as the basis for the regulation.\",\n",
        "  \"According to the service extracts from the health insurance, the insured has already been provided with the functional product requested according to its area of application.\",\n",
        "  \"A medically comprehensible explanation as to why the use of an orthopedic aid corresponding to the findings is not sufficient and instead electric foot lifter stimulation for walking would be more appropriate and therefore necessary has not been transmitted.\",\n",
        "  \"From an overall view of the information available here, it cannot be seen how the supply of the insured with the product could be justified, nor can the safety of such a supply be confirmed.\",\n",
        "  \"A medical justification for why a product not listed in the directory of aids should be used in the present case has not been transmitted.\"\n",
        "]"
      ],
      "metadata": {
        "id": "8mu2wI-9TVYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_de = [\n",
        "  \"Bei der hier benannten Diagnose ist das Erfordernis eines Ausgleichs zur Sicherstellung des Grundbedürfnisses denkbar.\",\n",
        "  \"Die sozialmedizinischen Voraussetzungen für die verordnete Hilfsmittelversorgung sind erfüllt.\",\n",
        "  \"Alltagsrelevante Gebrauchsvorteile werden festgestellt.\",\n",
        "  \"Sozialmedizinische Indikation für das Hilfsmittel wird bestätigt.\",\n",
        "  \"Kontraindikationen wurden ausgeschlossen, es liegen keine Gegenanzeigen für die Verwendung des beantragten Hilfsmittels vor.\"\n",
        "]"
      ],
      "metadata": {
        "id": "PkFlXrwuuH_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_de = [\n",
        "  \"Aus der aktuell als verordnungsbegründend benannten Diagnose lässt sich kein konkreter Befund ableiten.\",\n",
        "  \"Gemäß den Leistungsauszügen der Krankenkasse ist der Versicherte bereits entsprechend dem Einsatzbereich des beantragten funktionellen Produkt versorgt.\",\n",
        "  \"Eine medizinisch nachvollziehbare Begründung, weshalb der Einsatz einer befundadäquaten orthopädietechnischen Hilfsmittelversorgung nicht ausreichend und stattdessen eine elektrische Fußheberstimulation zum Gehen zweckmäßiger und deshalb notwendig wäre, wurde nicht übermittelt.\",\n",
        "  \"In der Gesamtschau der hier vorliegenden Informationen kann nicht erkannt werden, wie die Versorgung des Versicherten mit dem Produkt begründet werden könnte, noch kann die Unbedenklichkeit einer solchen Versorgung bestätigt werden.\",\n",
        "  \"Eine ärztliche Begründung, warum im vorliegenden Fall ein nicht im Hilfsmittelverzeichnis gelistetes Produkt zum Einsatz kommen soll, wird nicht übermittelt.\"\n",
        "]"
      ],
      "metadata": {
        "id": "K2D3Y5K1uJpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if lang == \"de\":\n",
        "  negative = negative_de\n",
        "  positive = positive_de\n",
        "else:\n",
        "  negative = negative_en\n",
        "  positive = positive_en\n",
        "\n"
      ],
      "metadata": {
        "id": "8Mg3YZG2t-Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criteria"
      ],
      "metadata": {
        "id": "o_8dOmMpjt0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PII on all contexts"
      ],
      "metadata": {
        "id": "OgOnGInRjx4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\\n\".join(positive + negative)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNCI_OXbDnck",
        "outputId": "2951c5a7-2406-482a-e896-c34cfefdea90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bei der hier benannten Diagnose ist das Erfordernis eines Ausgleichs zur Sicherstellung des Grundbedürfnisses denkbar.\n",
            "Die sozialmedizinischen Voraussetzungen für die verordnete Hilfsmittelversorgung sind erfüllt.\n",
            "Alltagsrelevante Gebrauchsvorteile werden festgestellt.\n",
            "Sozialmedizinische Indikation für das Hilfsmittel wird bestätigt.\n",
            "Kontraindikationen wurden ausgeschlossen, es liegen keine Gegenanzeigen für die Verwendung des beantragten Hilfsmittels vor.\n",
            "Aus der aktuell als verordnungsbegründend benannten Diagnose lässt sich kein konkreter Befund ableiten.\n",
            "Gemäß den Leistungsauszügen der Krankenkasse ist der Versicherte bereits entsprechend dem Einsatzbereich des beantragten funktionellen Produkt versorgt.\n",
            "Eine medizinisch nachvollziehbare Begründung, weshalb der Einsatz einer befundadäquaten orthopädietechnischen Hilfsmittelversorgung nicht ausreichend und stattdessen eine elektrische Fußheberstimulation zum Gehen zweckmäßiger und deshalb notwendig wäre, wurde nicht übermittelt.\n",
            "In der Gesamtschau der hier vorliegenden Informationen kann nicht erkannt werden, wie die Versorgung des Versicherten mit dem Produkt begründet werden könnte, noch kann die Unbedenklichkeit einer solchen Versorgung bestätigt werden.\n",
            "Eine ärztliche Begründung, warum im vorliegenden Fall ein nicht im Hilfsmittelverzeichnis gelistetes Produkt zum Einsatz kommen soll, wird nicht übermittelt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Oliver Zeigermann, geboren 22.12.1890 in Hamburg\"\n",
        "# text = \"geboren 22.12.1890 in Hamburg\"\n",
        "# text = \"aus Hamburg-Ottensen\""
      ],
      "metadata": {
        "id": "-F-PTDuvGeT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.evidentlyai.com/blog/open-source-llm-evaluation#llm-as-a-judge\n",
        "\n",
        "criteria = '''\n",
        "Personally identifiable information (PII) is information that, when used alone or with other relevant data, can identify an individual.\n",
        "\n",
        "PII may contain direct identifiers (e.g., passport information) that can identify a person uniquely or quasi-identifiers (e.g., race) that can be combined with other quasi-identifiers (e.g., date of birth) to successfully recognize an individual.\n",
        "PII may contain a person's name, person's address, and something I may forget to mention.\n",
        "\n",
        "Please identify whether or not the text below contains PII. Be strict, even a single identifier may be enough.\n",
        "'''\n",
        "\n",
        "PROMPT = f'''\n",
        "Evaluate the given criteria and generate a JSON that adheres to the given pydantic schema.\n",
        "\n",
        "# Text\n",
        "{text}\n",
        "\n",
        "# Criteria\n",
        "{criteria}\n",
        "\n",
        "# Pydantic Schema\n",
        "{str(Evaluation.schema())}\n",
        "\n",
        "# JSON Response\n",
        "'''\n",
        "\n",
        "# print(PROMPT)\n",
        "generate(model, tokenizer, PROMPT, Evaluation)"
      ],
      "metadata": {
        "id": "qsSAKyOtAwtZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1221e7b4-17e5-4118-c99b-8dd619cbca36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Evaluation(score=1.0, reasoning=\"The text contains Oliver Zeigermann's name, which is a direct identifier, and his date of birth, which can be used in combination with other data to identify an individual. Therefore, the criteria for personally identifiable information (PII) is met.\")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final inspection of memory, how much did the context window eat up\n",
        "\n",
        "* not to be confused with the assessment context\n",
        "* this is technical\n",
        "* composed of everything that is sent to the LLM inclusing system prompt, /  input question and assessment context\n",
        "\n",
        "Phi models take a lot of memory with growing context, Llama much more modest\n"
      ],
      "metadata": {
        "id": "jUUveM-Ej2Vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOf_Fv1PELPp",
        "outputId": "465cab00-02eb-448b-e722-f4a2efdcb017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug 24 16:17:26 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0              34W /  70W |   8417MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F9rfwWPPKMOY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}