{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNBbpIlYOIL4HsHfEYcPQ3X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fedacbc36d464bfab91091273df58b91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ec4c4d91c804249a7c37107f9decd4b",
              "IPY_MODEL_b231993e8d1b4662b9a019b568710d99",
              "IPY_MODEL_db33ff013db24a23b6bafacbb593d38a"
            ],
            "layout": "IPY_MODEL_eb4f2a84f608454eb564bb4644c3af64"
          }
        },
        "3ec4c4d91c804249a7c37107f9decd4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83bafd37f5d74473a6abfa6e1cfcf7ee",
            "placeholder": "​",
            "style": "IPY_MODEL_be9277ec967f4626ad7b29c5b5889abd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b231993e8d1b4662b9a019b568710d99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4b6ade1db384a9e9eef45fd88f3217c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9fb117c6d754a1099c93dfbf0198738",
            "value": 2
          }
        },
        "db33ff013db24a23b6bafacbb593d38a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f19d356fdace454aa689bbe7630dbcd6",
            "placeholder": "​",
            "style": "IPY_MODEL_469ff96762e6473a8b948cd56d48fc2f",
            "value": " 2/2 [00:03&lt;00:00,  1.59s/it]"
          }
        },
        "eb4f2a84f608454eb564bb4644c3af64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83bafd37f5d74473a6abfa6e1cfcf7ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be9277ec967f4626ad7b29c5b5889abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4b6ade1db384a9e9eef45fd88f3217c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9fb117c6d754a1099c93dfbf0198738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f19d356fdace454aa689bbe7630dbcd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "469ff96762e6473a8b948cd56d48fc2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJCordhose/practical-llm/blob/main/Assessment_Phi_3_mini_T4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assessment on Phi 3 mini\n",
        "\n",
        "* https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n",
        "* https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/\n",
        "\n",
        "Prerequisites\n",
        "1. a Huggingface account and an\n",
        "1. access tokens to put in below while logging in\n",
        "\n",
        "### Comparing GPU microarchitectures\n",
        "* T4/RTX 20: https://en.wikipedia.org/wiki/Turing_(microarchitecture)\n",
        "  * V100 - professional variant of RTX 20 consumer line: https://en.wikipedia.org/wiki/Volta_(microarchitecture)\n",
        "* A100/RTX 30: https://en.wikipedia.org/wiki/Ampere_(microarchitecture)\n",
        "* L4/L40/RTX 40: https://en.wikipedia.org/wiki/Ada_Lovelace_(microarchitecture)\n",
        "  * H100 - professional variant of RTX 40 consumer line, not available on Colab (yet?): https://en.wikipedia.org/wiki/Hopper_(microarchitecture)\n",
        "* Future successor to both Hopper and Ada Lovelace: https://en.wikipedia.org/wiki/Blackwell_(microarchitecture)\n",
        "* comparing GPUs: https://www.reddit.com/r/learnmachinelearning/comments/18gn1b2/choosing_the_right_gpu_for_your_workloads_a_dive/\n"
      ],
      "metadata": {
        "id": "iR6_IfMp4Pvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!pip install -q transformers accelerate flash_attn"
      ],
      "metadata": {
        "id": "g2EJQSHYFIjq",
        "outputId": "1b13a3fa-e27b-4dfe-d6af-f5211f5da42f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 32 ms, sys: 3.79 ms, total: 35.8 ms\n",
            "Wall time: 5.22 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "y-SYiDVt4Oxd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "pBA0hHok7bmG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGy0cCQZphdu",
        "outputId": "d6d1e35d-4e6e-458f-eaf2-334122562106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) \n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pEQyik-rrhL",
        "outputId": "1eb3f18a-1728-4593-b611-298deb9c4eb7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jul 18 13:54:19 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "transformers.__version__\n"
      ],
      "metadata": {
        "id": "xrP1Ao7OEy4A",
        "outputId": "a26c0f1e-6e92-4b1f-84c6-63261ffd0cea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.42.4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\""
      ],
      "metadata": {
        "id": "q5mV16Pt1NU6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYDzjGdHVXaG",
        "outputId": "b1d1ca27-8fb6-40ba-88ce-6c07cc380f74"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***note:*** execute in a termial 'watch -n 0.5 nvidia-smi' to see the GPU usage and when the model is loaded onto it"
      ],
      "metadata": {
        "id": "miDHXD2eVMXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    # quantization_config=quantization_config, # quantized 8-Bit for T4\n",
        "    torch_dtype=torch.bfloat16,  # 16-Bit full resolution\n",
        "    device_map=\"cuda\",\n",
        "    # https://huggingface.co/microsoft/Phi-3-mini-4k-instruct#hardware\n",
        "    attn_implementation=\"eager\", # remove on newer hardware\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "fedacbc36d464bfab91091273df58b91",
            "3ec4c4d91c804249a7c37107f9decd4b",
            "b231993e8d1b4662b9a019b568710d99",
            "db33ff013db24a23b6bafacbb593d38a",
            "eb4f2a84f608454eb564bb4644c3af64",
            "83bafd37f5d74473a6abfa6e1cfcf7ee",
            "be9277ec967f4626ad7b29c5b5889abd",
            "c4b6ade1db384a9e9eef45fd88f3217c",
            "d9fb117c6d754a1099c93dfbf0198738",
            "f19d356fdace454aa689bbe7630dbcd6",
            "469ff96762e6473a8b948cd56d48fc2f"
          ]
        },
        "id": "SK7DWCkEsLdv",
        "outputId": "00d080d1-8d3d-4d3e-c53a-04ba5091d1b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fedacbc36d464bfab91091273df58b91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.97 s, sys: 1.29 s, total: 4.26 s\n",
            "Wall time: 5.05 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8oNcTA51SAm",
        "outputId": "12c2bf93-94c2-4552-bec7-72ba7600e325"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jul 18 13:54:28 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P0              28W /  70W |   7393MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive = [\n",
        "  \"With the diagnosis named here, the need for compensation to ensure the basic need is conceivable.\",\n",
        "  \"The socio-medical prerequisites for the prescribed aid supply have been met.\",\n",
        "  \"Everyday relevant usage benefits have been determined.\",\n",
        "  \"Socio-medical indication for the aid is confirmed.\",\n",
        "  \"Contraindications have been excluded; there are no contraindications for the use of the requested aid.\"\n",
        "]"
      ],
      "metadata": {
        "id": "-V0LQbkdTDoD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative = [\n",
        "  \"No specific findings can be derived from the diagnosis currently named as the basis for the regulation.\",\n",
        "  \"According to the service extracts from the health insurance, the insured has already been provided with the functional product requested according to its area of application.\",\n",
        "  \"A medically comprehensible explanation as to why the use of an orthopedic aid corresponding to the findings is not sufficient and instead electric foot lifter stimulation for walking would be more appropriate and therefore necessary has not been transmitted.\",\n",
        "  \"From an overall view of the information available here, it cannot be seen how the supply of the insured with the product could be justified, nor can the safety of such a supply be confirmed.\",\n",
        "  \"A medical justification for why a product not listed in the directory of aids should be used in the present case has not been transmitted.\"\n",
        "]"
      ],
      "metadata": {
        "id": "8mu2wI-9TVYU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assessment = negative[0]\n",
        "# assessment = positive[0]"
      ],
      "metadata": {
        "id": "YGUixjbJTa6D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an English-speaking, competent expert in the field of statutory health insurance. Answer consice, serious and formal.\"},\n",
        "    {\"role\": \"user\", \"content\": f'''\n",
        "What is the result of the assessment? Is a positive or negative recommendation given? Answer with \"Yes\" or \"No\" and then provide a brief justification for your assessment.\n",
        "\n",
        "# Assessment\n",
        "{assessment}\n",
        "\n",
        "'''},\n",
        "]\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=512,\n",
        "    eos_token_id=terminators,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    do_sample=False\n",
        ")\n",
        "response = outputs[0][input_ids.shape[-1]:]\n",
        "Markdown(tokenizer.decode(response, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "CoxgSA_Xz0oN",
        "outputId": "a91a7d6f-8c9b-4e57-c18d-08be7568f640"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.c1358f8a35e6d2af81890deffbbfa575b978c62f.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.82 s, sys: 341 ms, total: 4.16 s\n",
            "Wall time: 4.16 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "No. The assessment indicates that there are no specific findings from the diagnosis that can be used as a basis for the regulation. This suggests that the regulation may not be well-founded or sufficiently supported by evidence, leading to a negative recommendation."
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_assessment(assessment):\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an English-speaking, competent expert in the field of statutory health insurance. Answer consice, serious and formal.\"},\n",
        "    {\"role\": \"user\", \"content\": f'''\n",
        "What is the result of the assessment? Is a positive or negative recommendation given? Answer with \"Yes\" or \"No\" and then provide a brief justification for your assessment.\n",
        "\n",
        "# Assessment\n",
        "{assessment}\n",
        "\n",
        "'''},\n",
        "]\n",
        "\n",
        "  input_ids = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\"\n",
        "  ).to(model.device)\n",
        "\n",
        "  terminators = [\n",
        "      tokenizer.eos_token_id,\n",
        "      tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  ]\n",
        "\n",
        "  outputs = model.generate(\n",
        "      input_ids,\n",
        "      max_new_tokens=512,\n",
        "      eos_token_id=terminators,\n",
        "      pad_token_id=tokenizer.eos_token_id,\n",
        "      do_sample=False\n",
        "  )\n",
        "  response = outputs[0][input_ids.shape[-1]:]\n",
        "  result = tokenizer.decode(response, skip_special_tokens=True)\n",
        "  if result.startswith(\"Yes\"):\n",
        "    return \"Positive\", result\n",
        "  elif result.startswith(\"No\"):\n",
        "    return \"Negative\", result\n",
        "  else:\n",
        "    return \"Neutral\", result"
      ],
      "metadata": {
        "id": "kS5CfvlVTSjo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Negative"
      ],
      "metadata": {
        "id": "reHbxlx1VGcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for assessment in negative:\n",
        "  print(f\"Assessment: {assessment}\")\n",
        "  result, explanation = eval_assessment(assessment)\n",
        "  print(f\"{result}: {explanation}\")\n",
        "  print(\"-----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otADLjgLTwZB",
        "outputId": "51a6b8dc-a16a-44bd-85ee-e9a715404c5b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assessment: No specific findings can be derived from the diagnosis currently named as the basis for the regulation.\n",
            "Negative: No. The assessment indicates that there are no specific findings from the diagnosis that can be used as a basis for the regulation. This suggests that the regulation may not be well-founded or sufficiently supported by evidence, leading to a negative recommendation.\n",
            "-----\n",
            "Assessment: According to the service extracts from the health insurance, the insured has already been provided with the functional product requested according to its area of application.\n",
            "Positive: Yes.\n",
            "\n",
            "The assessment indicates that the insured has been provided with the functional product as per the service extracts from the health insurance. This suggests that the insured's needs have been met, and there is no immediate need for additional products or services in this area.\n",
            "-----\n",
            "Assessment: A medically comprehensible explanation as to why the use of an orthopedic aid corresponding to the findings is not sufficient and instead electric foot lifter stimulation for walking would be more appropriate and therefore necessary has not been transmitted.\n",
            "Negative: No. The assessment indicates that the recommendation for an orthopedic aid is not sufficient based on the findings. Instead, the use of an electric foot lifter for walking is deemed more appropriate and necessary. The justification for this recommendation likely stems from a detailed analysis of the patient's condition, which suggests that the orthopedic aid alone would not adequately address the patient's mobility needs. The electric foot lifter, on the other hand, provides a more effective solution for walking assistance, as it can offer better support and control, potentially leading to improved patient outcomes.\n",
            "-----\n",
            "Assessment: From an overall view of the information available here, it cannot be seen how the supply of the insured with the product could be justified, nor can the safety of such a supply be confirmed.\n",
            "Negative: No.\n",
            "\n",
            "The assessment indicates that there is insufficient information to justify the supply of the product to the insured, and the safety of such a supply cannot be confirmed. This suggests a negative recommendation due to the lack of justification and safety confirmation.\n",
            "-----\n",
            "Assessment: A medical justification for why a product not listed in the directory of aids should be used in the present case has not been transmitted.\n",
            "Negative: No. The assessment indicates that there is a lack of medical justification for the use of a product not listed in the directory of aids. This suggests that the recommendation would likely be negative, as the product's inclusion in the directory typically requires evidence of its efficacy and appropriateness for the patient's condition.\n",
            "-----\n",
            "CPU times: user 17.2 s, sys: 291 ms, total: 17.5 s\n",
            "Wall time: 17.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positive"
      ],
      "metadata": {
        "id": "2bBV2H-UVIuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for assessment in positive:\n",
        "  print(f\"Assessment: {assessment}\")\n",
        "  result, explanation = eval_assessment(assessment)\n",
        "  print(f\"{result}: {explanation}\")\n",
        "  print(\"-----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3jE_bwWVJlu",
        "outputId": "4e57e9b2-218c-47b0-8abc-a43c84a08983"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assessment: With the diagnosis named here, the need for compensation to ensure the basic need is conceivable.\n",
            "Positive: Yes.\n",
            "\n",
            "The assessment indicates that there is a conceivable need for compensation to ensure the basic need, which suggests a positive recommendation for statutory health insurance coverage.\n",
            "-----\n",
            "Assessment: The socio-medical prerequisites for the prescribed aid supply have been met.\n",
            "Positive: Yes. The assessment indicates that the socio-medical prerequisites for the prescribed aid supply have been met, which suggests that the recommendation for the aid supply would be positive. This is because meeting the prerequisites is a necessary condition for the aid to be considered appropriate and justified.\n",
            "-----\n",
            "Assessment: Everyday relevant usage benefits have been determined.\n",
            "Positive: Yes. The assessment indicates that the benefits of everyday relevant usage have been determined, which suggests that the evaluation process has been completed and the findings are available. This implies a positive recommendation as it shows progress in the assessment process.\n",
            "-----\n",
            "Assessment: Socio-medical indication for the aid is confirmed.\n",
            "Positive: Yes. The assessment indicates a positive recommendation because the socio-medical indication for aid has been confirmed, which typically means that the individual meets the criteria for receiving assistance based on their medical and social needs.\n",
            "-----\n",
            "Assessment: Contraindications have been excluded; there are no contraindications for the use of the requested aid.\n",
            "Positive: Yes. The assessment indicates that there are no contraindications for the use of the requested aid, which suggests a positive recommendation for its use.\n",
            "-----\n",
            "CPU times: user 11.3 s, sys: 287 ms, total: 11.6 s\n",
            "Wall time: 11.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZr-hfVH6Nsx",
        "outputId": "e1c2b48d-bb23-49d4-9d54-c954436208cc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jul 18 13:55:02 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0              70W /  70W |   7615MiB / 15360MiB |     85%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    }
  ]
}