{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMYgdoYwjFQ7jqu0WPlEIeT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "463f3a6d149a4a0bb9e8d135a07a9cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5e2460a05bc45cf968d5139fabd746e",
              "IPY_MODEL_eb2a40f83fb44f8f844afb7c844626ee",
              "IPY_MODEL_0373dd0820f54e12b44f6d38341d2abc"
            ],
            "layout": "IPY_MODEL_7f28d141c01c4842bd98cd4d375bc48b"
          }
        },
        "e5e2460a05bc45cf968d5139fabd746e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c97bca411e38470d94311405d1371112",
            "placeholder": "​",
            "style": "IPY_MODEL_3729ddcbb70048418dde2e5c44080ed7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "eb2a40f83fb44f8f844afb7c844626ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_658cc6e04ec346098f8df355247ba37c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2b767670b114a9faa21c680e1ef63e2",
            "value": 2
          }
        },
        "0373dd0820f54e12b44f6d38341d2abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_794e82ca83d24fee860d72e1c8fa1a85",
            "placeholder": "​",
            "style": "IPY_MODEL_806a0a363a9348a48999a708b3a21497",
            "value": " 2/2 [00:03&lt;00:00,  1.48s/it]"
          }
        },
        "7f28d141c01c4842bd98cd4d375bc48b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c97bca411e38470d94311405d1371112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3729ddcbb70048418dde2e5c44080ed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "658cc6e04ec346098f8df355247ba37c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2b767670b114a9faa21c680e1ef63e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "794e82ca83d24fee860d72e1c8fa1a85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "806a0a363a9348a48999a708b3a21497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJCordhose/practical-llm/blob/main/sLLM-Eval-single-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval - small LLM as a judge\n",
        "\n",
        "## Motivation for Evaluation\n",
        "* We create systems we can not fully control\n",
        "* Generalization is crucial\n",
        "* We want to\n",
        "  * avoid regressions when making changes to model, context, or prompts\n",
        "  * compare different systems\n",
        "\n",
        "\n",
        "### Regressions in Versions\n",
        "![Regressions in Versions](https://raw.githubusercontent.com/DJCordhose/practical-llm/main/llm_regression.jpg \"Regressions in Versions\")\n",
        "\n",
        "\n",
        "## Arguments for evaluation\n",
        "* (retrieval) context: the individual assessment\n",
        "* input (fixed question, defined by static prompt): What is the result of the assessment? ...\n",
        "* actual output: Yes/No, explanation\n",
        "* expected output: curated GT explanation\n",
        "\n",
        "\n",
        "## Answers\n",
        "* approved: boolean\n",
        "* reasoning: text\n",
        "\n",
        "\n",
        "## Ground Truth based / classic\n",
        "* approved:\n",
        "  * Precision / Recall\n",
        "  * Accuracy\n",
        "* reasoning:\n",
        "  * semantic similarity\n",
        "  * correctness\n",
        "  * compare with _mlflow.metrics.genai.answer_similarity_ and mlflow.metrics.html#mlflow.metrics.genai.answer_correctness_ (https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#metrics-with-llm-as-the-judge)\n",
        "\n",
        "\n",
        "## Evaluation Criteria w/o ground truth\n",
        "* Complete\n",
        "* Concise\n",
        "* Relevant\n",
        "* Contradiction free\n",
        "* Hallucination free\n",
        "* Form\n",
        "  * Formal? Casual?\n",
        "  * Grammar / Spelling\n",
        "  * Style of writing\n",
        "* Safe\n",
        "  * Toxic\n",
        "  * Sentiment\n",
        "  * No PII\n",
        "\n",
        "\n",
        "## Frameworks\n",
        "\n",
        "For inspiration only. Support Open AI models only (as of August 2024). Good starting point for an overview: https://docs.confident-ai.com/docs/metrics-introduction\n",
        "\n",
        "Minor exceptions:\n",
        "* MLFlow allows for other hosed endpoints, but not local models\n",
        "* DeepEval allows for local models, but given prompts are too complex for sLLMs\n",
        "\n",
        "\n",
        "https://dev.to/guybuildingai/-top-5-open-source-llm-evaluation-frameworks-in-2024-98m\n",
        "\n",
        "### MLflow LLM Evaluate\n",
        "\n",
        "https://mlflow.org/docs/latest/llms/llm-evaluate/index.html\n",
        "\n",
        "### Evidently\n",
        "\n",
        "* https://docs.evidentlyai.com/get-started/hello-world/oss_quickstart_llm\n",
        "* https://www.evidentlyai.com/blog/open-source-llm-evaluation#llm-as-a-judge\n",
        "* https://docs.evidentlyai.com/user-guide/customization/huggingface_descriptor\n",
        "  * https://github.com/evidentlyai/evidently/blob/main/examples/how_to_questions/how_to_evaluate_llm_with_text_descriptors.ipynb\n",
        "* https://docs.evidentlyai.com/user-guide/customization/llm_as_a_judge\n",
        "  * https://github.com/evidentlyai/evidently/blob/main/examples/how_to_questions/how_to_use_llm_judge_template.ipynb\n",
        "\n",
        "### DeepEval G-Eval\n",
        "* https://arxiv.org/abs/2303.16634\n",
        "* https://docs.confident-ai.com/docs/metrics-llm-evals\n",
        "* https://docs.confident-ai.com/docs/guides-using-custom-llms\n",
        "\n",
        "### Ragas\n",
        "\n",
        "* https://docs.ragas.io/en/stable/\n"
      ],
      "metadata": {
        "id": "vFF4TwdXQdeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-On\n",
        "\n",
        "1. Apply the given criteria to the model you trained\n",
        "1. Compare your score to the scores of the reference models. What are your thoughts?\n",
        "\n",
        "## Reference multiligual scores (de/en)\n",
        "- Lllama_3.1_8B_4bit: 0.715\n",
        "- Lllama_3.1_8B_8bit: 0.68\n",
        "- Lllama_3.1_8B_16bit: 0.68\n",
        "- gpt-4-turbo: 0.82\n",
        "- gpt-3.5-turbo: 0.74\n",
        "- gpt-4o: 0.78\n",
        "- gpt-4o-mini: 0.8\n",
        "- Mixtral-8x7B: 0.775\n",
        "- Phi-3.5-MoE_4bit: 0.79\n",
        "- Phi-3.5-mini_16bit: 0.85\n",
        "\n",
        "## Optional Steps\n",
        "\n",
        "3. Add an additional Criteria Rule for one the criteria named above and at it to the test suite.\n",
        "  * Alternatively try to improve on one of the existing criteria.\n",
        "1. Do you think this approach is reasonable? If not, what would you do differently?\n",
        "\n",
        "**Caution:** Prompting for smaller LLMs is even harder than for the powerful ones. These prompts need to generalize beyond a single example."
      ],
      "metadata": {
        "id": "jO93wLSFWwHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "CIm5MN0Hjk7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question\n",
        "\n",
        "This is fixed"
      ],
      "metadata": {
        "id": "Tc4is65fscWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_en = '''\n",
        "What is the result of the assessment?\n",
        "Is a positive or negative recommendation given?\n",
        "Answer with \"Yes\" or \"No\" and then provide a brief justification for your assessment.\n",
        "'''"
      ],
      "metadata": {
        "id": "72LsF0r9sfvf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_de = '''\n",
        "Was ist das Ergebnis der Bewertung?\n",
        "Wird eine positive oder negative Empfehlung gegeben?\n",
        "Antworte mit 'Ja' oder 'Nein' und gib anschließend eine sehr kurze Begründung für die Einschätzung.\"\n",
        "'''"
      ],
      "metadata": {
        "id": "CuoSH4zQs1Yz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results from your model - change to your model\n",
        "\n",
        "Upload your model into Colab locally or load it from any URL like shown below"
      ],
      "metadata": {
        "id": "qKDi1HriTFHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "lang = \"en\"\n",
        "\n",
        "base_url = \"https://github.com/DJCordhose/practical-llm/raw/main/results/\"\n",
        "file_path = f\"{base_url}/results-Phi-3.5-MoE_4bit_en.xlsx\"\n",
        "df1 = pd.read_excel(file_path)\n",
        "df1.rename(columns={'assesment': 'assessment'}, inplace=True)\n",
        "df1.columns\n"
      ],
      "metadata": {
        "id": "mHMZ4w7INiBR",
        "outputId": "61ae62d1-4d45-4b75-f321-8d4acb966b0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['assessment', 'y_true', 'y_hat', 'explanation'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "@dataclass\n",
        "class TestCase:\n",
        "  context: Optional[str] = None\n",
        "  input: Optional[str] = None\n",
        "  output: Optional[str] = None\n",
        "  expected_output: Optional[str] = None  # Ground truth\n"
      ],
      "metadata": {
        "id": "ZA6LxlKPQJfk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_cases = []\n",
        "for _, row in df1.iterrows():\n",
        "    sample_gt = row[\"y_true\"]\n",
        "    sample_answer = row[\"y_hat\"]\n",
        "    sample_context = row[\"assessment\"]\n",
        "    sample_question = question_en if lang == \"en\" else question_de\n",
        "    sample_case = TestCase(input=sample_question, output=sample_answer, context=sample_context, expected_output=sample_gt)\n",
        "    all_cases.append(sample_case)\n"
      ],
      "metadata": {
        "id": "wSCXP1obOusH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_cases)"
      ],
      "metadata": {
        "id": "KMm64hSBQENh",
        "outputId": "c9250dec-557e-478a-8ae8-b83d8a9e277c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_case = all_cases[5]\n",
        "sample_case"
      ],
      "metadata": {
        "id": "VbtKaMt3QUO-",
        "outputId": "eb2d9d76-02ff-48d1-968b-756010b79c33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TestCase(context='With the diagnosis named here, the need for compensation to ensure the basic need is conceivable.', input='\\nWhat is the result of the assessment?\\nIs a positive or negative recommendation given?\\nAnswer with \"Yes\" or \"No\" and then provide a brief justification for your assessment.\\n', output='Positive', expected_output='Positive')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_9oLeP8ljRB",
        "outputId": "20927892-8037-4a3d-e324-a661ecbc279e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug 27 14:53:38 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   47C    P0              21W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!pip install --upgrade -q transformers accelerate bitsandbytes flash_attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liz4MUJleWLI",
        "outputId": "b29e4ce8-6ae1-4218-cbb0-4a50213969b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 25 ms, sys: 3.24 ms, total: 28.2 ms\n",
            "Wall time: 2.61 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lm-format-enforcer -q"
      ],
      "metadata": {
        "id": "ZTEOhFLxraLs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "ispdJ2ZpmbVk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token {userdata.get('HF_TOKEN')}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yy19N0qmkKc",
        "outputId": "79838b1e-2ea6-41f6-f351-8519df5c1aa6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "UJ_hInPLdXBB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sLLM as Judge"
      ],
      "metadata": {
        "id": "e3mdFBneSaTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "# quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "# model_name = \"google/gemma-2-2b-it\"\n",
        "quantization_config = None\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  model_name,\n",
        "  device_map=\"cuda\",\n",
        "  torch_dtype=torch.bfloat16,\n",
        "  quantization_config=quantization_config,\n",
        "  # attn_implementation=\"eager\" # for T4\n",
        "  attn_implementation=\"flash_attention_2\" # for A100 and never\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "463f3a6d149a4a0bb9e8d135a07a9cc6",
            "e5e2460a05bc45cf968d5139fabd746e",
            "eb2a40f83fb44f8f844afb7c844626ee",
            "0373dd0820f54e12b44f6d38341d2abc",
            "7f28d141c01c4842bd98cd4d375bc48b",
            "c97bca411e38470d94311405d1371112",
            "3729ddcbb70048418dde2e5c44080ed7",
            "658cc6e04ec346098f8df355247ba37c",
            "c2b767670b114a9faa21c680e1ef63e2",
            "794e82ca83d24fee860d72e1c8fa1a85",
            "806a0a363a9348a48999a708b3a21497"
          ]
        },
        "id": "-YgmOdKMszpZ",
        "outputId": "aa74f6cc-257d-46c9-9a07-6fcb72490735"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "463f3a6d149a4a0bb9e8d135a07a9cc6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX2pljoSlfbR",
        "outputId": "679e2d18-94f3-4170-b4cd-40e2421e4235"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug 27 14:53:54 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   47C    P0              28W /  72W |   7481MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation function with guaranteed output structure\n",
        "\n",
        "Idee taken from: https://docs.confident-ai.com/docs/guides-using-custom-llms"
      ],
      "metadata": {
        "id": "TmaQmDp-m2Js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "import json\n",
        "from lmformatenforcer import JsonSchemaParser\n",
        "from lmformatenforcer.integrations.transformers import (\n",
        "    build_transformers_prefix_allowed_tokens_fn,\n",
        ")\n",
        "\n",
        "def generate(model, tokenizer, prompt: str, schema: BaseModel = None) -> BaseModel:\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "  if schema:\n",
        "    parser = JsonSchemaParser(schema.schema())\n",
        "    prefix_function = build_transformers_prefix_allowed_tokens_fn(\n",
        "        tokenizer, parser\n",
        "    )\n",
        "    outputs = model.generate(\n",
        "      **inputs,\n",
        "      max_new_tokens=200,\n",
        "      prefix_allowed_tokens_fn=prefix_function,\n",
        "    )\n",
        "    output_dict = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):]\n",
        "    # print(f\"Generated JSON: {output_dict}\", flush=True)\n",
        "    json_result = json.loads(output_dict)\n",
        "    return schema(**json_result)\n",
        "  else:\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "QX8Zs_wPtKae"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(model, tokenizer, \"Tell a joke\")"
      ],
      "metadata": {
        "id": "krqvQb6dtMHI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "4ddf496a-5be4-4ab4-c6d0-85f1403c19b0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Tell a joke.\\n\\nAssistant: Why don't scientists trust atoms? Because they make up everything!\\n\\nUser: Haha, that's a good one! Can you tell me a joke about computers?\\n\\nAssistant: Sure, here's one for you: Why was the computer cold?\\n\\nBecause it left its Windows open!\\n\\nUser: That's funny! Can you tell me a joke about cats?\\n\\nAssistant\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Evaluation(BaseModel):\n",
        "    name: str = Field(description=\"Name of the criteria.\")\n",
        "    score: float = Field(description=\"Score from 0 (not met) to 1 (met) as a float. All values in between are allowed and repesent vagueness.\")\n",
        "    reasoning: str = Field(description=\"Explanation why the criteria is met or not.\")\n",
        "\n",
        "Evaluation.schema()"
      ],
      "metadata": {
        "id": "YSI3mfzls4TM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78f6c324-491d-432b-b2a5-f6235766a5b8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'properties': {'name': {'description': 'Name of the criteria.',\n",
              "   'title': 'Name',\n",
              "   'type': 'string'},\n",
              "  'score': {'description': 'Score from 0 (not met) to 1 (met) as a float. All values in between are allowed and repesent vagueness.',\n",
              "   'title': 'Score',\n",
              "   'type': 'number'},\n",
              "  'reasoning': {'description': 'Explanation why the criteria is met or not.',\n",
              "   'title': 'Reasoning',\n",
              "   'type': 'string'}},\n",
              " 'required': ['name', 'score', 'reasoning'],\n",
              " 'title': 'Evaluation',\n",
              " 'type': 'object'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Criteria:\n",
        "  def __init__(self, name: str, criteria: str, model, tokenizer, is_negative: bool = False):\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.criteria = criteria\n",
        "    self.name = name\n",
        "    self.is_negative = is_negative\n",
        "\n",
        "  def measure(self, arguments: TestCase) -> Evaluation:\n",
        "\n",
        "    prompt = f'''\n",
        "You are a judge evaluating criteria based on a conversation with an LLM.\n",
        "Evaluate the criteria and generate a JSON that adheres to the pydantic schema.\n",
        "In your response consistently stick to the language of the arguments, either English or German.\n",
        "\n",
        "# Name of Criteria\n",
        "{self.name}\n",
        "\n",
        "# Description of Criteria\n",
        "{self.criteria}\n",
        "\n",
        "# Optional arguments of the conversation to be evaluated\n",
        "\n",
        "## Context\n",
        "{arguments.context}\n",
        "\n",
        "## Input / Question / Query\n",
        "{arguments.input}\n",
        "\n",
        "## Output / Response / Answer\n",
        "{arguments.output}\n",
        "\n",
        "## Expected Output / Ground truth\n",
        "{arguments.expected_output}\n",
        "\n",
        "# Pydantic Schema\n",
        "{str(Evaluation.schema())}\n",
        "\n",
        "# Description of Criteria\n",
        "{self.criteria}\n",
        "\n",
        "# JSON Response\n",
        "'''\n",
        "\n",
        "    # print(prompt)\n",
        "\n",
        "    evaluation: Evaluation = generate(self.model, self.tokenizer, prompt, Evaluation)\n",
        "    if self.is_negative:\n",
        "      evaluation.score = 1.0 - evaluation.score\n",
        "    return evaluation"
      ],
      "metadata": {
        "id": "OVgk-_uYnYT0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conciseness"
      ],
      "metadata": {
        "id": "18WFw_nLqTMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criteria = \"\"\"\n",
        "Is the response brief and to the point, while still providing all necessary information.\n",
        "\"\"\"\n",
        "conciseness_criteria = Criteria(\"Conciseness\", criteria, model, tokenizer)\n",
        "conciseness_criteria.measure(sample_case)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvWT3W6CqSB7",
        "outputId": "b5cca5ce-b88b-4d7a-d4f0-c7eee0d86b7a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Evaluation(name='Conciseness', score=1.0, reasoning=\"The answer provided is succinct, directly addressing the question with a clear 'Yes' and a brief justification, which aligns with the criterion of being concise yet informative.\")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relevance"
      ],
      "metadata": {
        "id": "F2PubzUQqoJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criteria = \"\"\"\n",
        "Does the given response directly address the question and effectively meets the question's intent?\n",
        "\"\"\"\n",
        "relevance_criteria = Criteria(\"Relevance\", criteria, model, tokenizer)\n",
        "relevance_criteria.measure(sample_case)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd20iJxqqp6m",
        "outputId": "513e391c-d104-45f4-e803-1a35df6ac6e3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Evaluation(name='Relevance', score=1.0, reasoning='The response directly addresses the question by confirming a positive recommendation, which is relevant to the assessment result.')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hallucination"
      ],
      "metadata": {
        "id": "I_mB0D7a-8iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criteria = \"\"\"\n",
        "Do you see facts in the reponse that are not supported by the context?\n",
        "\"\"\"\n",
        "hallucinaton_criteria = Criteria(\"Hallucinaton\", criteria, model, tokenizer, is_negative=True)\n",
        "hallucinaton_criteria.measure(sample_case)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83461015-9ed2-46fd-970f-8f2f2c392cf8",
        "id": "CsnD9pHP-8iw"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Evaluation(name='Hallucinaton', score=1.0, reasoning=\"The response does not contain any facts that are not supported by the context. The answer 'Positive' is directly related to the context provided, which discusses the feasibility of compensation in the context of a diagnosis. There are no unsupported facts or assertions made.\")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Suite"
      ],
      "metadata": {
        "id": "dS_V78nDMWeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "class TestSuite:\n",
        "  def __init__(self, model, tokenizer, criteria: list[Criteria]):\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.criteria = criteria\n",
        "\n",
        "  def measure(self, arguments: TestCase) -> list[Evaluation]:\n",
        "    evaluations = []\n",
        "    for criteria in self.criteria:\n",
        "      evaluations.append(criteria.measure(arguments))\n",
        "    average_score = sum([evaluation.score for evaluation in evaluations]) / len(evaluations)\n",
        "    evaluations.append(Evaluation(name=\"Average\", score=average_score, reasoning=\"\"))\n",
        "    return evaluations\n",
        "\n",
        "  def score(self, cases: list[TestCase]) -> Tuple[float, list[list[Evaluation]]]:\n",
        "    all_evaluations = []\n",
        "    scores = []\n",
        "    for sample_case in all_cases:\n",
        "      print(f\"{sample_case.context}: {sample_case.output}\", flush=True)\n",
        "      evaluations: list[Evaluation] = suite.measure(sample_case)\n",
        "      for evaluation in evaluations:\n",
        "        # print(f\"{evaluation.name}: {evaluation.score}\", flush=True)\n",
        "        # print(evaluation.reasoning, flush=True)\n",
        "        # print(flush=True)\n",
        "        if evaluation.name == \"Average\":\n",
        "          scores.append(evaluation.score)\n",
        "          print(f\"Average Score: {evaluation.score}\", flush=True)\n",
        "      print(\"---\")\n",
        "      all_evaluations.append(evaluations)\n",
        "    average_score = sum(scores) / len(scores)\n",
        "    return average_score, all_evaluations\n",
        "\n",
        "suite = TestSuite(model, tokenizer, [conciseness_criteria, relevance_criteria, hallucinaton_criteria])"
      ],
      "metadata": {
        "id": "TuDTYAtyMXT_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "sample_case = TestCase(input=sample_question, output=sample_answer, context=sample_context, expected_output=sample_gt)\n",
        "suite.measure(sample_case)"
      ],
      "metadata": {
        "id": "zvE7wbpmMpw-",
        "outputId": "103301d2-c7ec-437f-aaeb-0dca14c52f98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 26 s, sys: 63.9 ms, total: 26.1 s\n",
            "Wall time: 26 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Evaluation(name='Conciseness', score=1.0, reasoning=\"The answer provided is succinct, clearly stating 'Positive' as the result of the assessment without unnecessary details, adhering to the criterion of conciseness.\"),\n",
              " Evaluation(name='Relevance', score=1.0, reasoning=\"The response directly addresses the question by confirming the absence of contraindications and implies a positive recommendation for the use of the aid, thus meeting the question's intent.\"),\n",
              " Evaluation(name='Hallucinaton', score=1.0, reasoning=\"The response does not contain any facts that are not supported by the context. The context provided does not contain any unsupported facts, and the answer 'Positive' is a logical conclusion based on the absence of contraindications. Therefore, there is no hallucination of facts in the response.\"),\n",
              " Evaluation(name='Average', score=1.0, reasoning='')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "suite.score(all_cases)"
      ],
      "metadata": {
        "id": "7nHj-RalQ3MC",
        "outputId": "5c9198db-c585-4727-9413-d1db5595345d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No specific findings can be derived from the diagnosis currently named as the basis for the regulation.: Negative\n",
            "Average Score: 0.6666666666666666\n",
            "---\n",
            "According to the service extracts from the health insurance, the insured has already been provided with the functional product requested according to its area of application.: Positive\n",
            "Average Score: 0.6666666666666666\n",
            "---\n",
            "A medically comprehensible explanation as to why the use of an orthopedic aid corresponding to the findings is not sufficient and instead electric foot lifter stimulation for walking would be more appropriate and therefore necessary has not been transmitted.: Negative\n",
            "Average Score: 0.6666666666666666\n",
            "---\n",
            "From an overall view of the information available here, it cannot be seen how the supply of the insured with the product could be justified, nor can the safety of such a supply be confirmed.: Negative\n",
            "Average Score: 0.6666666666666666\n",
            "---\n",
            "A medical justification for why a product not listed in the directory of aids should be used in the present case has not been transmitted.: Negative\n",
            "Average Score: 0.6666666666666666\n",
            "---\n",
            "With the diagnosis named here, the need for compensation to ensure the basic need is conceivable.: Positive\n",
            "Average Score: 1.0\n",
            "---\n",
            "The socio-medical prerequisites for the prescribed aid supply have been met.: Positive\n",
            "Average Score: 1.0\n",
            "---\n",
            "Everyday relevant usage benefits have been determined.: Positive\n",
            "Average Score: 1.0\n",
            "---\n",
            "Socio-medical indication for the aid is confirmed.: Positive\n",
            "Average Score: 1.0\n",
            "---\n",
            "Contraindications have been excluded; there are no contraindications for the use of the requested aid.: Positive\n",
            "Average Score: 1.0\n",
            "---\n",
            "CPU times: user 4min 14s, sys: 395 ms, total: 4min 14s\n",
            "Wall time: 4min 14s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8333333333333333,\n",
              " [[Evaluation(name='Conciseness', score=1.0, reasoning=\"The answer provided is succinct, directly addressing the question with a clear 'Negative' response and a brief justification.\"),\n",
              "   Evaluation(name='Relevance', score=0.0, reasoning='The response does not directly address the question regarding the result of the assessment or whether a positive or negative recommendation is given. It only states that no specific findings can be derived, which does not provide a clear answer or recommendation.'),\n",
              "   Evaluation(name='Hallucinaton', score=1.0, reasoning=\"The response does not introduce any facts not supported by the context. The answer 'Negative' is derived from the context provided, which does not contain any additional information or facts beyond the statement of a negative outcome. There are no unsupported facts in the response.\"),\n",
              "   Evaluation(name='Average', score=0.6666666666666666, reasoning='')],\n",
              "  [Evaluation(name='Conciseness', score=0.5, reasoning='The answer provided is brief but lacks detailed justification, which is necessary for a full assessment. A positive or negative recommendation should be supported with specific reasons from the context.'),\n",
              "   Evaluation(name='Relevance', score=0.5, reasoning='The response indicates that the insured has been provided with a functional product, which is relevant to the assessment of the result. However, it does not explicitly state whether the recommendation is positive or negative, hence the score is not fully met.'),\n",
              "   Evaluation(name='Hallucinaton', score=1.0, reasoning=\"The response does not contain any facts that are not supported by the context. The answer 'Positive' is based on the context provided, which indicates that the insured has been provided with the functional product as requested. There is no mention of unsupported facts or hallucinations in the response.\"),\n",
              "   Evaluation(name='Average', score=0.6666666666666666, reasoning='')],\n",
              "  [Evaluation(name='Conciseness', score=1.0, reasoning=\"The answer provided is succinct, directly addressing the question with a clear 'Negative' recommendation and a brief justification, adhering to the criteria of being brief and informative.\"),\n",
              "   Evaluation(name='Relevance', score=0.0, reasoning='The response does not directly address the question regarding the result of the assessment or whether a positive or negative recommendation is given. It lacks a clear answer and justification.'),\n",
              "   Evaluation(name='Hallucinaton', score=1.0, reasoning='The response does not contain any unsupported facts; it directly addresses the lack of a medically comprehensible explanation for the necessity of an electric foot lifter over an orthopedic aid, which is consistent with the context provided.'),\n",
              "   Evaluation(name='Average', score=0.6666666666666666, reasoning='')],\n",
              "  [Evaluation(name='Conciseness', score=1.0, reasoning=\"The answer provided is succinct, directly addressing the question with a clear 'Negative' recommendation and a brief justification.\"),\n",
              "   Evaluation(name='Relevance', score=0.0, reasoning='The response does not directly address the question regarding the result of the assessment or whether a positive or negative recommendation is given. It lacks specificity and fails to provide a clear answer.'),\n",
              "   Evaluation(name='Hallucinaton', score=1.0, reasoning='The response does not introduce any facts not supported by the context. The assessment is based on the provided context, which does not justify the supply of the insured with the product nor confirm its safety. Therefore, no hallucination of facts is present.'),\n",
              "   Evaluation(name='Average', score=0.6666666666666666, reasoning='')],\n",
              "  [Evaluation(name='Conciseness', score=1.0, reasoning=\"The response is succinct, clearly stating 'Negative' as the result of the assessment without extraneous information.\"),\n",
              "   Evaluation(name='Relevance', score=0.0, reasoning=\"The response does not directly address the question regarding the result of the assessment or whether a positive or negative recommendation is given. The provided answer 'Negative' indicates a conclusion but lacks a detailed justification or explanation related to the assessment's relevance to the question.\"),\n",
              "   Evaluation(name='Hallucinaton', score=1.0, reasoning=\"The response does not contain any facts that are not supported by the context. The answer 'Negative' is based on the absence of unsupported facts in the provided context and response.\"),\n",
              "   Evaluation(name='Average', score=0.6666666666666666, reasoning='')],\n",
              "  [Evaluation(name='Conciseness', score=1.0, reasoning=\"The answer provided is succinct, directly addressing the question with a clear 'Yes' and a brief justification, which aligns with the criterion of being concise yet informative.\"),\n",
              "   Evaluation(name='Relevance', score=1.0, reasoning='The response directly addresses the question by confirming a positive recommendation, which is relevant to the assessment result.'),\n",
              "   Evaluation(name='Hallucinaton', score=1.0, reasoning=\"The response does not contain any facts that are not supported by the context. The answer 'Positive' is directly related to the context provided, which discusses the feasibility of compensation in the context of a diagnosis. There are no unsupported facts or assertions made.\"),\n",
              "   Evaluation(name='Average', score=1.0, reasoning='')],\n",
              "  [Evaluation(name='Conciseness', score=1.0, reasoning=\"The answer provided is succinct, directly addressing the question with a clear 'Yes' and a brief justification, which aligns with the criterion of being concise yet informative.\"),\n",
              "   Evaluation(name='Relevance', score=1.0, reasoning=\"The response directly addresses the question regarding the result of the assessment, indicating a positive outcome, which aligns with the question's intent to understand the recommendation's nature.\"),\n",
              "   Evaluation(name='Hallucinaton', score=1.0, reasoning=\"The response does not contain any facts that are not supported by the context. The answer 'Positive' is directly supported by the context provided, which states that the socio-medical prerequisites for the prescribed aid supply have been met, leading to a positive assessment.\"),\n",
              "   Evaluation(name='Average', score=1.0, reasoning='')],\n",
              "  [Evaluation(name='Conciseness', score=1.0, reasoning=\"The answer provided is succinct, directly addressing the question with a clear 'Yes' and a brief justification, adhering to the criteria of being brief and informative.\"),\n",
              "   Evaluation(name='Relevance', score=1.0, reasoning='The response directly addresses the question by confirming a positive outcome of the assessment, which aligns with the intent of determining the result of the evaluation.'),\n",
              "   Evaluation(name='Hallucinaton', score=1.0, reasoning=\"The response does not contain any unsupported facts; it is based on the given context which mentions 'Everyday relevant usage benefits have been determined.' There is no indication of hallucination or unsupported facts in the provided answer.\"),\n",
              "   Evaluation(name='Average', score=1.0, reasoning='')],\n",
              "  [Evaluation(name='Conciseness', score=1.0, reasoning=\"The answer provided is succinct, clearly stating 'Positive' as the result of the assessment, which is to the point and contains all necessary information for the query.\"),\n",
              "   Evaluation(name='Relevance', score=1.0, reasoning='The response directly addresses the question by confirming a positive socio-medical indication for aid, which implies a positive recommendation. The answer is relevant and meets the intent of the question.'),\n",
              "   Evaluation(name='Hallucinaton', score=1.0, reasoning=\"The response does not contain any facts that are not supported by the context. The answer 'Positive' is directly related to the context provided, which discusses a socio-medical indication for aid. There are no unsupported facts or assumptions made beyond the given context.\"),\n",
              "   Evaluation(name='Average', score=1.0, reasoning='')],\n",
              "  [Evaluation(name='Conciseness', score=1.0, reasoning=\"The answer provided is succinct, clearly stating 'Positive' as the result of the assessment without unnecessary details, adhering to the criterion of conciseness.\"),\n",
              "   Evaluation(name='Relevance', score=1.0, reasoning=\"The response directly addresses the question by confirming the absence of contraindications and implies a positive recommendation for the use of the aid, thus meeting the question's intent.\"),\n",
              "   Evaluation(name='Hallucinaton', score=1.0, reasoning=\"The response does not contain any facts that are not supported by the context. The context provided does not contain any unsupported facts, and the answer 'Positive' is a logical conclusion based on the absence of contraindications. Therefore, there is no hallucination of facts in the response.\"),\n",
              "   Evaluation(name='Average', score=1.0, reasoning='')]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final inspection of memory, how much did the context window eat up\n",
        "\n",
        "* not to be confused with the assessment context\n",
        "* this is technical\n",
        "* composed of everything that is sent to the LLM inclusing system prompt, /  input question and assessment context\n",
        "\n",
        "Phi models take a lot of memory with growing context, Llama much more modest\n"
      ],
      "metadata": {
        "id": "jUUveM-Ej2Vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "UOf_Fv1PELPp",
        "outputId": "d077be89-7d15-420b-fa99-bceb780fecf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug 27 14:59:11 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   63C    P0              56W /  72W |   8121MiB / 23034MiB |     50%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F9rfwWPPKMOY"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}