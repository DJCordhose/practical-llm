{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b2092b2f0bfd40979281c0ecf8443d63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d46f13fdecf64e1da826bacfb40dc945",
              "IPY_MODEL_cb7fd0c5bc4c4ca58f441304d26a44ea",
              "IPY_MODEL_402da27616bd48e8bb197f946681c434"
            ],
            "layout": "IPY_MODEL_c43c6024ca474a60bfc3dbc6a63e1c39"
          }
        },
        "d46f13fdecf64e1da826bacfb40dc945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af0eb34278f748c0a75ee7ea694d7689",
            "placeholder": "​",
            "style": "IPY_MODEL_d6e1a2029e524959a95544864b35e800",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cb7fd0c5bc4c4ca58f441304d26a44ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e2c47040754441bada4370b1ced40f5",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2be5d0ea2da144b7ac99909f2ee3febb",
            "value": 4
          }
        },
        "402da27616bd48e8bb197f946681c434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a95521ba5a14554841c156800cc30da",
            "placeholder": "​",
            "style": "IPY_MODEL_f153ca8b67704f3198c36d2b550f0633",
            "value": " 4/4 [01:25&lt;00:00, 18.43s/it]"
          }
        },
        "c43c6024ca474a60bfc3dbc6a63e1c39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af0eb34278f748c0a75ee7ea694d7689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6e1a2029e524959a95544864b35e800": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e2c47040754441bada4370b1ced40f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2be5d0ea2da144b7ac99909f2ee3febb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a95521ba5a14554841c156800cc30da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f153ca8b67704f3198c36d2b550f0633": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ff432efeaef478da3f4555105c12d0b": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_2ffa1c2ab7f74e95bfb6710e7ab340da",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mLanguage (GEval) Metric\u001b[0m! \u001b[38;2;55;65;81m(using meta-llama/Meta-Llama-3.1-8B-Instruct, strict=…\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Language (GEval) Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using meta-llama/Meta-Llama-3.1-8B-Instruct, strict=…</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "2ffa1c2ab7f74e95bfb6710e7ab340da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "397f315c01594675ae913aae5899f607": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_2643cc499cae432bad3b37024e6c4140",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mConciseness (GEval) Metric\u001b[0m! \u001b[38;2;55;65;81m(using meta-llama/Meta-Llama-3.1-8B-Instruct, stri…\u001b[0m\n✨ You're running DeepEval's latest \u001b[38;2;106;0;255mLanguage (GEval) Metric\u001b[0m! \u001b[38;2;55;65;81m(using meta-llama/Meta-Llama-3.1-8B-Instruct, strict=…\u001b[0m\n✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using meta-llama/Meta-Llama-3.1-8B-Instruct, strict=…\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Conciseness (GEval) Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using meta-llama/Meta-Llama-3.1-8B-Instruct, stri…</span>\n✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Language (GEval) Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using meta-llama/Meta-Llama-3.1-8B-Instruct, strict=…</span>\n✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using meta-llama/Meta-Llama-3.1-8B-Instruct, strict=…</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "2643cc499cae432bad3b37024e6c4140": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "160c648b756c4ab8990cf9b24c6b51b1": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_638b37b240ae4552a42edc30da96982d",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mConciseness (GEval) Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m   \n✨ You're running DeepEval's latest \u001b[38;2;106;0;255mLanguage (GEval) Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m \u001b[38;2;25;227;160mDone…\u001b[0m\n✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using gpt-4o, strict=False, async_mode=True)...\u001b[0m \u001b[38;2;25;227;160mDone…\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Conciseness (GEval) Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span>   \n✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Language (GEval) Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span> <span style=\"color: #19e3a0; text-decoration-color: #19e3a0\">Done…</span>\n✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using gpt-4o, strict=False, async_mode=True)...</span> <span style=\"color: #19e3a0; text-decoration-color: #19e3a0\">Done…</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "638b37b240ae4552a42edc30da96982d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJCordhose/practical-llm/blob/main/Eval4pptx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands on: Eval - small LLM as a judge\n",
        "\n",
        "Goal\n",
        "* see how llm-as-a-judge works in principle\n",
        "* introduction to the G-Eval algorithm ([G-Eval on arxive ](https://arxiv.org/abs/2303.16634))\n",
        "* see how the algorithm uses prompts to generate the actual eval prompt\n",
        "* try out the [DeepEval library](https://docs.confident-ai.com/docs/guides-using-custom-llms)\n",
        "* see the limitations of small llms as a judge\n",
        "* optional: compare to Gpt-4o"
      ],
      "metadata": {
        "id": "vFF4TwdXQdeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SetUp : create an *llm_run* method using a small LLM\n",
        "\n",
        "* load & quantize a small model from huggingface\n",
        "* define a simple **llm_run** method, that calls the loaded model\n",
        "* try out llm_run\n",
        "\n",
        "=> same setup as in Assement notebook"
      ],
      "metadata": {
        "id": "HUWl-0x-524k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC0CYqfnOg_j",
        "outputId": "22d9cb35-a976-4e72-ddee-427956fe90a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Sep  3 11:03:08 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0              28W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Important:**\n",
        "Ensure that no GPU memory is allocated yet (in the case of a T4 look for \"0MiB / 15360MiB\").\n",
        "If GPU memory is already allocated use Runtime/Manage Sessions to delete all active sessions."
      ],
      "metadata": {
        "id": "waEtPWk5MjyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "wnUuUSG68wlB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!pip install --upgrade -q transformers accelerate flash_attn torch bitsandbytes\n",
        "!pip install lm-format-enforcer -q\n",
        "!pip install deepeval==1.1.1 -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liz4MUJleWLI",
        "outputId": "8ff05b02-7664-4c2f-89d8-5dd44b6fa655"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 35.5 ms, sys: 5.14 ms, total: 40.6 ms\n",
            "Wall time: 8.34 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### => you may need to restart the session"
      ],
      "metadata": {
        "id": "zdHUEIcDBdw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Configure HuggingFace token as a Colab Secret, use key symbol on the left panel\n",
        "!huggingface-cli login --token {userdata.get('HF_TOKEN')}"
      ],
      "metadata": {
        "id": "ispdJ2ZpmbVk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27df11e1-6f8b-472c-8053-a60038c99b78"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load & quantize Model (yielding: model_id, model, tokenizer)"
      ],
      "metadata": {
        "id": "PL_RxgOy58BG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# kind = 'Lllama_3.1_8B_4bit'\n",
        "kind = 'Lllama_3.1_8B_8bit'\n",
        "# kind = 'Lllama_3.1_8B_16bit'  # too large for T4\n",
        "# kind = 'Phi-3.5-MoE_4bit'     # No module named 'triton' ???\n",
        "# kind = \"Phi-3.5-mini_16bit\"   # not \"strong\" enough\n",
        "\n",
        "if \"Lllama_3.1_8B\" in kind:\n",
        "  model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "elif \"Phi-3.5-MoE\" in kind:\n",
        "  model_id = \"microsoft/Phi-3.5-MoE-instruct\"\n",
        "else:\n",
        "  model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "\n",
        "print(kind)\n",
        "print(model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyH5zHti6A-S",
        "outputId": "d79266b1-8ffb-47f3-fda6-e50a4afafde3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lllama_3.1_8B_8bit\n",
            "meta-llama/Meta-Llama-3.1-8B-Instruct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***note:*** execute in a terminal 'watch -n 0.5 nvidia-smi' to see the GPU usage and when the model is loaded onto it"
      ],
      "metadata": {
        "id": "-7yP74Cl6Omy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "torch_dtype = None\n",
        "quantization_config = None\n",
        "\n",
        "if \"8bit\" in kind:\n",
        "  print(\"Using 8Bit quantization\")\n",
        "  quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "elif \"4bit\" in kind:\n",
        "  print(\"Using 4Bit quantization\")\n",
        "  quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "else:\n",
        "  print(\"Using Full Resolution\")\n",
        "  torch_dtype = torch.bfloat16\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device_map=\"cuda\",\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "b2092b2f0bfd40979281c0ecf8443d63",
            "d46f13fdecf64e1da826bacfb40dc945",
            "cb7fd0c5bc4c4ca58f441304d26a44ea",
            "402da27616bd48e8bb197f946681c434",
            "c43c6024ca474a60bfc3dbc6a63e1c39",
            "af0eb34278f748c0a75ee7ea694d7689",
            "d6e1a2029e524959a95544864b35e800",
            "8e2c47040754441bada4370b1ced40f5",
            "2be5d0ea2da144b7ac99909f2ee3febb",
            "3a95521ba5a14554841c156800cc30da",
            "f153ca8b67704f3198c36d2b550f0633"
          ]
        },
        "id": "-YgmOdKMszpZ",
        "outputId": "a4fabdb9-06c2-4b9b-e136-7dca84ef90dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 8Bit quantization\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2092b2f0bfd40979281c0ecf8443d63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 18.9 s, sys: 16.9 s, total: 35.9 s\n",
            "Wall time: 1min 31s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "NX2pljoSlfbR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20fa7b36-f13c-4f9e-a9d8-6ccdd5e0fa84"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Sep  3 11:04:50 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0              28W /  70W |   8825MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "def llm_run(messages):\n",
        "  if type(messages) == str:\n",
        "    messages = [{\"role\": \"user\", \"content\": messages}]\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "  terminators = [\n",
        "      tokenizer.eos_token_id,\n",
        "      tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  ]\n",
        "\n",
        "  input_token_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        "  ).to(model.device)\n",
        "\n",
        "  outputs = model.generate(\n",
        "      input_token_ids,\n",
        "      max_new_tokens=512,\n",
        "      eos_token_id=terminators,\n",
        "      pad_token_id=tokenizer.eos_token_id,\n",
        "      do_sample=False\n",
        "  )\n",
        "  output_token_ids = outputs[0][input_token_ids.shape[-1]:]\n",
        "  result = tokenizer.decode(output_token_ids, skip_special_tokens=True)\n",
        "  return result"
      ],
      "metadata": {
        "id": "jlJxNTGs6xM3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try out our model:"
      ],
      "metadata": {
        "id": "8b-8riV2Vf_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(model_id)\n",
        "llm_run(\"who are you ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "nf93aT-JVPol",
        "outputId": "38efd960-bd51-4567-c932-7a14fc6e62cb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "meta-llama/Meta-Llama-3.1-8B-Instruct\n",
            "CPU times: user 5.47 s, sys: 38 ms, total: 5.51 s\n",
            "Wall time: 5.82 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an English-speaking, competent expert in the field of sanitary piping systems..\"},\n",
        "    {\"role\": \"user\", \"content\": f'''What are waste-water pipes made out of ?'''}\n",
        "  ]\n",
        "\n",
        "#answer = llm_run(messages)\n",
        "#Markdown(answer)"
      ],
      "metadata": {
        "id": "hGxdDNNY6e8H"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM-as-a-judge: in principle"
      ],
      "metadata": {
        "id": "z0jYXr3F9pem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_output=\"Witing texts is painful, caus im making mitakes.\""
      ],
      "metadata": {
        "id": "Zh74JhHgQOMl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_eval_prompt = f'''\n",
        "You are an expert on english language, grading a students text with scores between 0 and 10.\n",
        "A text written in proper english, in a fluent style, containing no grammatical or syntax errors is graded 10.\n",
        "A text written in a different language or with spelling errors gets a low score.\n",
        "Also give a detailed explanation why the score was chosen.\n",
        "Do not repeat the students text in your explanation.\n",
        "\n",
        "Always answer in the following json format:\n",
        "{{\n",
        "    \"score\": 8,\n",
        "    \"reason\": \"some reason\"\n",
        "}}\n",
        "\n",
        "Examples\n",
        "1. Student Text: Pipes are cylindrical conduits used to transport fluids or gases, typically made of materials like metal, plastic, or concrete.\n",
        "   Answer:\n",
        "   {{\n",
        "    \"score\": 8,\n",
        "    \"reason\": \"The text is written in english and does not contain any syntactical or grammatical erros\"\n",
        "  }}\n",
        "2. Student Text: Zwischen Neonlichtern und Straßenlärm träum ich leise von Freiheit.\n",
        "   Answer:\n",
        "   {{\n",
        "    \"score\": 2,\n",
        "    \"reason\": \"The text is written in german and not in english.\"\n",
        "  }}\n",
        "\n",
        "Student Text: {llm_output}\n",
        "Answer:\n",
        "'''"
      ],
      "metadata": {
        "id": "1fUQwWQd8W2D"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "print(\"***** Prompt         :\")\n",
        "print(simple_eval_prompt)\n",
        "\n",
        "answer = llm_run(simple_eval_prompt)\n",
        "\n",
        "print(\"***** Answer         :\")\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "90PhBKcH8Wu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e9b83d8-c4ae-4e5b-fcaf-55ab62b54914"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Prompt         :\n",
            "\n",
            "You are an expert on english language, grading a students text with scores between 0 and 10.\n",
            "A text written in proper english, in a fluent style, containing no grammatical or syntax errors is graded 10.\n",
            "A text written in a different language or with spelling errors gets a low score.\n",
            "Also give a detailed explanation why the score was chosen.\n",
            "Do not repeat the students text in your explanation.\n",
            "\n",
            "Always answer in the following json format:\n",
            "{\n",
            "    \"score\": 8,\n",
            "    \"reason\": \"some reason\"\n",
            "}\n",
            "\n",
            "Examples\n",
            "1. Student Text: Pipes are cylindrical conduits used to transport fluids or gases, typically made of materials like metal, plastic, or concrete.\n",
            "   Answer:\n",
            "   {\n",
            "    \"score\": 8,\n",
            "    \"reason\": \"The text is written in english and does not contain any syntactical or grammatical erros\"\n",
            "  }\n",
            "2. Student Text: Zwischen Neonlichtern und Straßenlärm träum ich leise von Freiheit.\n",
            "   Answer:\n",
            "   {\n",
            "    \"score\": 2,\n",
            "    \"reason\": \"The text is written in german and not in english.\"\n",
            "  }\n",
            "\n",
            "Student Text: Witing texts is painful, caus im making mitakes.\n",
            "Answer:\n",
            "\n",
            "***** Answer         :\n",
            "{\n",
            "    \"score\": 1,\n",
            "    \"reason\": \"The text contains multiple spelling errors, such as 'Witing' instead of 'Writing', 'caus' instead of 'because', and'mitakes' instead of'mistakes'. Additionally, the text is written in a non-standard dialect of English, which affects its clarity and coherence.\"\n",
            "}\n",
            "CPU times: user 19 s, sys: 63.1 ms, total: 19.1 s\n",
            "Wall time: 20.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llm-as-a-judge: G-Eval in principal"
      ],
      "metadata": {
        "id": "xtY98fXd6gl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Idea:** given just a criteria use an llm to generate a detailed evaluation prompt.\n",
        "\n",
        "https://arxiv.org/pdf/2303.16634"
      ],
      "metadata": {
        "id": "BwT8zKTlWg05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### G-Eval Phase 1: generate evaluation steps based on the criteria"
      ],
      "metadata": {
        "id": "p4swZQuDG5GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criteria=\"Grade the english grammar and syntax\""
      ],
      "metadata": {
        "id": "fjD209UWH0Kw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "geval_phase1_prompt = f'''Given an evaluation criteria which outlines how you should judge the Actual Output, generate\n",
        "3-4 concise evaluation steps based on the criteria below. You MUST make it clear how to evaluate Actual Output in\n",
        "relation to one another.\n",
        "\n",
        "Evaluation Criteria:\n",
        "{criteria}\n",
        "\n",
        "**\n",
        "IMPORTANT: Please make sure to only return in JSON format, with the \"steps\" key as a list of strings. No words or\n",
        "explanation is needed.\n",
        "Example JSON:\n",
        "{{\n",
        "    \"steps\": <list_of_strings>\n",
        "}}\n",
        "**\n",
        "\n",
        "JSON:\n",
        "'''\n",
        "\n",
        "print(\"***** Prompt         :\")\n",
        "print(geval_phase1_prompt)\n",
        "\n",
        "answer_phase1 = llm_run(geval_phase1_prompt)\n",
        "\n",
        "print(\"***** Answer         :\")\n",
        "print(answer_phase1)\n"
      ],
      "metadata": {
        "id": "fo-nE5KcAVXP",
        "outputId": "f6af49aa-ceb5-4752-a0ee-68ccd1185c37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Prompt         :\n",
            "Given an evaluation criteria which outlines how you should judge the Actual Output, generate\n",
            "3-4 concise evaluation steps based on the criteria below. You MUST make it clear how to evaluate Actual Output in\n",
            "relation to one another.\n",
            "\n",
            "Evaluation Criteria:\n",
            "Grade the english grammar and syntax\n",
            "\n",
            "**\n",
            "IMPORTANT: Please make sure to only return in JSON format, with the \"steps\" key as a list of strings. No words or\n",
            "explanation is needed.\n",
            "Example JSON:\n",
            "{\n",
            "    \"steps\": <list_of_strings>\n",
            "}\n",
            "**\n",
            "\n",
            "JSON:\n",
            "\n",
            "***** Answer         :\n",
            "{\n",
            "  \"steps\": [\n",
            "    \"Compare the subject-verb agreement in the Actual Output with the expected output.\",\n",
            "    \"Evaluate the tense consistency and correct use of verb forms in the Actual Output.\",\n",
            "    \"Check for correct use of pronouns, articles, and prepositions in the Actual Output.\",\n",
            "    \"Assess the overall sentence structure and flow of the Actual Output for grammatical correctness.\"\n",
            "  ]\n",
            "}\n",
            "CPU times: user 17.5 s, sys: 61.7 ms, total: 17.6 s\n",
            "Wall time: 17.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "json_answer_step1=json.loads(answer_phase1)\n",
        "steps=\"\\n\".join(f\"{index+1}. {step}\" for index, step in enumerate(json_answer_step1['steps']))\n",
        "print(steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIwct7DYOdQp",
        "outputId": "25c764d7-38e4-4d45-8639-0c23f698f6df"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Compare the subject-verb agreement in the Actual Output with the expected output.\n",
            "2. Evaluate the tense consistency and correct use of verb forms in the Actual Output.\n",
            "3. Check for correct use of pronouns, articles, and prepositions in the Actual Output.\n",
            "4. Assess the overall sentence structure and flow of the Actual Output for grammatical correctness.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### G-Eval Phase 2: evaluate the llm_output using the generated steps"
      ],
      "metadata": {
        "id": "WuObfnA4HNF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_input=\"Why do you dislike writing texts ?\"\n",
        "llm_output=\"Witing texts is painful, caus im making mitakes.\""
      ],
      "metadata": {
        "id": "L34HVzfyXRC9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geval_phase2_prompt = f'''\n",
        "Given the evaluation steps, return a JSON with two keys:\n",
        "1) a `score` key ranging from 0 - 10, with 10 being that it follows the criteria outlined in the steps and 0 being that it does not, and\n",
        "2) a `reason` key, a reason for the given score, but DO NOT QUOTE THE SCORE in your reason.\n",
        "Please mention specific information from Actual Output and Input in your reason, but be very concise with it!\n",
        "\n",
        "Evaluation Steps:\n",
        "{steps}\n",
        "\n",
        "Actual Output:\n",
        "{llm_output}\n",
        "\n",
        "Input:\n",
        "{llm_input}\n",
        "\n",
        "\n",
        "\n",
        "**\n",
        "IMPORTANT: Please make sure to only return in JSON format, with the \"score\" and \"reason\" key. No words or explanation is needed.\n",
        "\n",
        "Example JSON:\n",
        "{{\n",
        "    \"score\": 0,\n",
        "    \"reason\": \"The text does not follow the evaluation steps provided.\"\n",
        "}}\n",
        "**\n",
        "\n",
        "JSON:\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "QjYjN-9oH6VG"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"***** Prompt         :\")\n",
        "print(geval_phase2_prompt)\n",
        "\n",
        "answer_phase2=llm_run(geval_phase2_prompt)\n",
        "print(\"***** Answer         :\")\n",
        "print(answer_phase2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pla-jaYNPxgt",
        "outputId": "8d9f2b87-3e3d-47d0-c838-43edff302df6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Prompt         :\n",
            "\n",
            "Given the evaluation steps, return a JSON with two keys:\n",
            "1) a `score` key ranging from 0 - 10, with 10 being that it follows the criteria outlined in the steps and 0 being that it does not, and\n",
            "2) a `reason` key, a reason for the given score, but DO NOT QUOTE THE SCORE in your reason.\n",
            "Please mention specific information from Actual Output and Input in your reason, but be very concise with it!\n",
            "\n",
            "Evaluation Steps:\n",
            "1. Compare the subject-verb agreement in the Actual Output with the expected output.\n",
            "2. Evaluate the tense consistency and correct use of verb forms in the Actual Output.\n",
            "3. Check for correct use of pronouns, articles, and prepositions in the Actual Output.\n",
            "4. Assess the overall sentence structure and flow of the Actual Output for grammatical correctness.\n",
            "\n",
            "Actual Output:\n",
            "Witing texts is painful, caus im making mitakes.\n",
            "\n",
            "Input:\n",
            "What is a pipe used for ?\n",
            "\n",
            "\n",
            "\n",
            "**\n",
            "IMPORTANT: Please make sure to only return in JSON format, with the \"score\" and \"reason\" key. No words or explanation is needed.\n",
            "\n",
            "Example JSON:\n",
            "{\n",
            "    \"score\": 0,\n",
            "    \"reason\": \"The text does not follow the evaluation steps provided.\"\n",
            "}\n",
            "**\n",
            "\n",
            "JSON:\n",
            "\n",
            "\n",
            "***** Answer         :\n",
            "{\n",
            "    \"score\": 0,\n",
            "    \"reason\": \"Subject-verb agreement is incorrect, tense is inconsistent, and there are spelling mistakes.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# G-Eval Implementation by DeepEval"
      ],
      "metadata": {
        "id": "cu3nSLWzG_JG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A little adapter to connect our **llm_run** function with the deepEval library (and do some logging)\n",
        "\n",
        "see: https://docs.confident-ai.com/docs/guides-using-custom-llms"
      ],
      "metadata": {
        "id": "Z9fEWGsY6v18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.models import DeepEvalBaseLLM\n",
        "\n",
        "log_output=\"\"\n",
        "llm_log = True\n",
        "\n",
        "def log(log_message):\n",
        "    global log_output\n",
        "\n",
        "    if llm_log:\n",
        "          log_output += log_message + \"\\n\"\n",
        "\n",
        "# wrapper calling llm_run using the global members model_id, model\n",
        "class CustomDeepEvalLlm(DeepEvalBaseLLM):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.generate_count = 0\n",
        "\n",
        "    def load_model(self):\n",
        "        return model\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        self.generate_count += 1\n",
        "        count = self.generate_count\n",
        "        log(f'[{count}] ********************** deepEval LLM Generate BEGIN ********************************************')\n",
        "        log(f'[{count}] ***** Prompt         : ' + prompt)\n",
        "\n",
        "        result = llm_run(prompt)\n",
        "\n",
        "        log(f'[{count}] ***** Answer         : ' + result)\n",
        "        log(f'[{count}] ********************** deepEval LLM Generate END   ')\n",
        "        log(f'[{count}] ')\n",
        "        return result\n",
        "\n",
        "    async def a_generate(self, prompt: str) -> str:\n",
        "        return self.generate(prompt)\n",
        "\n",
        "    def get_model_name(self):\n",
        "        return model_id\n",
        "\n",
        "deepeval_custom_model = CustomDeepEvalLlm()"
      ],
      "metadata": {
        "id": "pRgPt2He6fEO"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "try out the wrapper and check log output"
      ],
      "metadata": {
        "id": "mdaSg0H8WDp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deepeval_custom_model.generate('who are you ?')"
      ],
      "metadata": {
        "id": "B5Dq6gWrEU-5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6c845701-8e37-48fc-fcc2-407c7310fdf1"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(log_output)"
      ],
      "metadata": {
        "id": "5yvgkOIkLkzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8883605a-c2c9-4404-abad-5b6918279c26"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] ********************** deepEval LLM Generate BEGIN ********************************************\n",
            "[1] ***** Prompt         : who are you ?\n",
            "[1] ***** Answer         : I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n",
            "[1] ********************** deepEval LLM Generate END   \n",
            "[1] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**geval_run** calls deepEval's implementation passing our criteria, llm_input and llm_output"
      ],
      "metadata": {
        "id": "8hLIk5Zzp7jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import deepeval\n",
        "import deepeval.metrics\n",
        "import deepeval.test_case\n",
        "from deepeval.models.base_model import DeepEvalBaseLLM\n",
        "from deepeval.metrics import GEval\n",
        "from deepeval.test_case import LLMTestCaseParams\n",
        "\n",
        "def geval_run(name, criteria, input, output):\n",
        "    result = deepeval.evaluate(\n",
        "\n",
        "        test_cases=[deepeval.test_case.LLMTestCase(input=input, actual_output=output )],\n",
        "\n",
        "        metrics=[ GEval(\n",
        "              name=name,\n",
        "              criteria=criteria,\n",
        "              evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT,LLMTestCaseParams.INPUT],\n",
        "              model=deepeval_custom_model\n",
        "        )]\n",
        "    )\n",
        "    return result"
      ],
      "metadata": {
        "id": "2x3hftSQaea-"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "log_output=\"\"\n",
        "r = geval_run(\"Language\", \"Grade the english grammar and syntax.\", llm_input, llm_output )"
      ],
      "metadata": {
        "id": "s-eQQ9sYcCKq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575,
          "referenced_widgets": [
            "5ff432efeaef478da3f4555105c12d0b",
            "2ffa1c2ab7f74e95bfb6710e7ab340da"
          ]
        },
        "outputId": "3e2ac962-a0aa-47a0-c6df-92dcb0ae083e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ff432efeaef478da3f4555105c12d0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating test cases...\n",
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Language (GEval) (score: 0.0, threshold: 0.5, strict: False, evaluation model: meta-llama/Meta-Llama-3.1-8B-Instruct, reason: The text contains grammatical errors, such as 'Witing' instead of 'Writing', 'caus' instead of 'because', and'mitakes' instead of'mistakes'. The sentence structure and punctuation are also incorrect., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What is a pipe used for ?\n",
            "  - actual output: Witing texts is painful, caus im making mitakes.\n",
            "  - expected output: None\n",
            "  - context: None\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Language (GEval): 0.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "🎉 Tests finished ✅! Run \u001b[32m'deepeval login'\u001b[0m to view evaluation results on Confident AI. ‼️ NOTE: You can also run \n",
              "evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🎉 Tests finished ✅! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to view evaluation results on Confident AI. ‼️ NOTE: You can also run \n",
              "evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 43.1 s, sys: 533 ms, total: 43.6 s\n",
            "Wall time: 51.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_metrics_data(deep_eval_result):\n",
        "    for testcase in deep_eval_result:\n",
        "      print(\"input        :\", testcase.input)\n",
        "      print(\"actual_output:\", testcase.actual_output)\n",
        "      print()\n",
        "      for metric in testcase.metrics_data:\n",
        "        print(\"name         :\",metric.name)\n",
        "        print(\"score        :\",metric.score)\n",
        "        print(\"reason       :\",metric.reason)\n",
        "        print(\"model        :\",metric.evaluation_model)\n",
        "        print()\n",
        "      print(\"-----------\")"
      ],
      "metadata": {
        "id": "r-xjg5guXVmK"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_metrics_data(r)\n",
        "r"
      ],
      "metadata": {
        "id": "D7R_yZbFtx7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "131de933-ea7d-4ae2-9d7a-b42c92f2e059"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input        : What is a pipe used for ?\n",
            "actual_output: Witing texts is painful, caus im making mitakes.\n",
            "\n",
            "name         : Language (GEval)\n",
            "score        : 0.0\n",
            "reason       : The text contains grammatical errors, such as 'Witing' instead of 'Writing', 'caus' instead of 'because', and'mitakes' instead of'mistakes'. The sentence structure and punctuation are also incorrect.\n",
            "model        : meta-llama/Meta-Llama-3.1-8B-Instruct\n",
            "\n",
            "-----------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TestResult(success=False, metrics_data=[MetricData(name='Language (GEval)', threshold=0.5, success=False, score=0.0, reason=\"The text contains grammatical errors, such as 'Witing' instead of 'Writing', 'caus' instead of 'because', and'mitakes' instead of'mistakes'. The sentence structure and punctuation are also incorrect.\", strict_mode=False, evaluation_model='meta-llama/Meta-Llama-3.1-8B-Instruct', error=None, evaluation_cost=None, verbose_logs='Criteria:\\nGrade the english grammar and syntax. \\n \\nEvaluation Steps:\\n[\\n    \"Compare the Actual Output to the Input to identify any grammatical errors or inconsistencies.\",\\n    \"Evaluate the Actual Output\\'s sentence structure, verb tense, and subject-verb agreement in relation to the Input.\",\\n    \"Assess the Actual Output\\'s punctuation, capitalization, and spelling accuracy in comparison to the Input.\",\\n    \"Determine the overall grammatical correctness of the Actual Output by considering its alignment with the Input\\'s intended meaning and structure.\"\\n]')], input='What is a pipe used for ?', actual_output='Witing texts is painful, caus im making mitakes.', expected_output=None, context=None, retrieval_context=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(log_output)"
      ],
      "metadata": {
        "id": "u-sF6C_oM4cs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48945ce7-e781-4c39-81bb-66452bd326be"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2] ********************** deepEval LLM Generate BEGIN ********************************************\n",
            "[2] ***** Prompt         : Given an evaluation criteria which outlines how you should judge the Actual Output and Input, generate 3-4 concise evaluation steps based on the criteria below. You MUST make it clear how to evaluate Actual Output and Input in relation to one another.\n",
            "\n",
            "Evaluation Criteria:\n",
            "Grade the english grammar and syntax.\n",
            "\n",
            "**\n",
            "IMPORTANT: Please make sure to only return in JSON format, with the \"steps\" key as a list of strings. No words or explanation is needed.\n",
            "Example JSON:\n",
            "{\n",
            "    \"steps\": <list_of_strings>\n",
            "}\n",
            "**\n",
            "\n",
            "JSON:\n",
            "\n",
            "[2] ***** Answer         : {\n",
            "  \"steps\": [\n",
            "    \"Compare the Actual Output to the Input to identify any grammatical errors or inconsistencies.\",\n",
            "    \"Evaluate the Actual Output's sentence structure, verb tense, and subject-verb agreement in relation to the Input.\",\n",
            "    \"Assess the Actual Output's punctuation, capitalization, and spelling accuracy in comparison to the Input.\",\n",
            "    \"Determine the overall grammatical correctness of the Actual Output by considering its alignment with the Input's intended meaning and structure.\"\n",
            "  ]\n",
            "}\n",
            "[2] ********************** deepEval LLM Generate END   \n",
            "[2] \n",
            "[3] ********************** deepEval LLM Generate BEGIN ********************************************\n",
            "[3] ***** Prompt         : Given the evaluation steps, return a JSON with two keys: 1) a `score` key ranging from 0 - 10, with 10 being that it follows the criteria outlined in the steps and 0 being that it does not, and 2) a `reason` key, a reason for the given score, but DO NOT QUOTE THE SCORE in your reason. Please mention specific information from Actual Output and Input in your reason, but be very concise with it!\n",
            "\n",
            "Evaluation Steps:\n",
            "1. Compare the Actual Output to the Input to identify any grammatical errors or inconsistencies.\n",
            "2. Evaluate the Actual Output's sentence structure, verb tense, and subject-verb agreement in relation to the Input.\n",
            "3. Assess the Actual Output's punctuation, capitalization, and spelling accuracy in comparison to the Input.\n",
            "4. Determine the overall grammatical correctness of the Actual Output by considering its alignment with the Input's intended meaning and structure.\n",
            "\n",
            "\n",
            "Actual Output:\n",
            "Witing texts is painful, caus im making mitakes. \n",
            "\n",
            "Input:\n",
            "What is a pipe used for ? \n",
            "\n",
            "\n",
            "\n",
            "**\n",
            "IMPORTANT: Please make sure to only return in JSON format, with the \"score\" and \"reason\" key. No words or explanation is needed.\n",
            "\n",
            "Example JSON:\n",
            "{\n",
            "    \"score\": 0,\n",
            "    \"reason\": \"The text does not follow the evaluation steps provided.\"\n",
            "}\n",
            "**\n",
            "\n",
            "JSON:\n",
            "\n",
            "[3] ***** Answer         : {\n",
            "    \"score\": 0,\n",
            "    \"reason\": \"The text contains grammatical errors, such as 'Witing' instead of 'Writing', 'caus' instead of 'because', and'mitakes' instead of'mistakes'. The sentence structure and punctuation are also incorrect.\"\n",
            "}\n",
            "[3] ********************** deepEval LLM Generate END   \n",
            "[3] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating multiple metrics: \"Conciseness\", AnswerRelevance, Toxicity,..."
      ],
      "metadata": {
        "id": "u91oPU40-0sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "check out some other metrics [https://docs.confident-ai.com/docs/metrics-introduction](https://docs.confident-ai.com/docs/metrics-introduction)"
      ],
      "metadata": {
        "id": "o8TGFB0gWPjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deepeval_custom_model = CustomDeepEvalLlm()"
      ],
      "metadata": {
        "id": "K9V2JQKRc84b"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import AnswerRelevancyMetric, ToxicityMetric\n",
        "\n",
        "def metrics_run(input, output):\n",
        "    test_case = deepeval.test_case.LLMTestCase(\n",
        "        input=input,\n",
        "        actual_output=output\n",
        "      )\n",
        "\n",
        "    conciseness_metric = GEval(\n",
        "        name=\"Conciseness\",\n",
        "        criteria=\"Determine how concise the actual output is. Ignore the input.\",\n",
        "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.INPUT],\n",
        "        model=deepeval_custom_model\n",
        "    )\n",
        "\n",
        "    language_metric = GEval(\n",
        "        name=\"Language\",\n",
        "        criteria=\"Grade the english grammar and syntax. Ignore the input.\",\n",
        "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.INPUT],\n",
        "        model=deepeval_custom_model\n",
        "    )\n",
        "\n",
        "    metrics = [\n",
        "        conciseness_metric,\n",
        "        language_metric,\n",
        "        AnswerRelevancyMetric(model=deepeval_custom_model),\n",
        "        # ToxicityMetric(model=deepeval_custom_model) # Lllama_3.1_8B_8bit not \"strong\" enough\n",
        "    ]\n",
        "\n",
        "    eval_result = deepeval.evaluate(\n",
        "        test_cases=[test_case],\n",
        "        metrics=metrics,\n",
        "    )\n",
        "    return eval_result"
      ],
      "metadata": {
        "id": "4zCdsHgawTNW"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_input=\"What is a pipe used for ?\"\n",
        "llm_output_concise=\"A pipe is a tubular conduit used to transport fluids or sometimes solids.\"\n",
        "llm_output_inconcise=\"Pipes are beautiful, black and round. Because they are round they are very convenient and don't have any edges.\""
      ],
      "metadata": {
        "id": "RXbZChu78adp"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_output=\"\"\n",
        "deepeval_custom_model = CustomDeepEvalLlm()\n",
        "r=metrics_run(llm_input, llm_output_concise)\n",
        "print_metrics_data(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989,
          "referenced_widgets": [
            "397f315c01594675ae913aae5899f607",
            "2643cc499cae432bad3b37024e6c4140"
          ]
        },
        "id": "5u-IvdmpCKBI",
        "outputId": "70a47406-3450-460b-c946-03c830621b8e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "397f315c01594675ae913aae5899f607"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating test cases...\n",
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Conciseness (GEval) (score: 0.8, threshold: 0.5, strict: False, evaluation model: meta-llama/Meta-Llama-3.1-8B-Instruct, reason: The actual output is concise and conveys necessary information without unnecessary details, but it could be more clear in its definition of a pipe., error: None)\n",
            "  - ✅ Language (GEval) (score: 0.9, threshold: 0.5, strict: False, evaluation model: meta-llama/Meta-Llama-3.1-8B-Instruct, reason: The text follows the evaluation steps, except for a minor capitalization error in the first word., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: meta-llama/Meta-Llama-3.1-8B-Instruct, reason: The score is 1.00 because the actual output directly addresses the question about the use of a pipe, making it highly relevant., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What is a pipe used for ?\n",
            "  - actual output: A pipe is a tubular conduit used to transport fluids or sometimes solids.\n",
            "  - expected output: None\n",
            "  - context: None\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Conciseness (GEval): 100.00% pass rate\n",
            "Language (GEval): 100.00% pass rate\n",
            "Answer Relevancy: 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "🎉 Tests finished ✅! Run \u001b[32m'deepeval login'\u001b[0m to view evaluation results on Confident AI. ‼️ NOTE: You can also run \n",
              "evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🎉 Tests finished ✅! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to view evaluation results on Confident AI. ‼️ NOTE: You can also run \n",
              "evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input        : What is a pipe used for ?\n",
            "actual_output: A pipe is a tubular conduit used to transport fluids or sometimes solids.\n",
            "\n",
            "name         : Conciseness (GEval)\n",
            "score        : 0.8\n",
            "reason       : The actual output is concise and conveys necessary information without unnecessary details, but it could be more clear in its definition of a pipe.\n",
            "model        : meta-llama/Meta-Llama-3.1-8B-Instruct\n",
            "\n",
            "name         : Language (GEval)\n",
            "score        : 0.9\n",
            "reason       : The text follows the evaluation steps, except for a minor capitalization error in the first word.\n",
            "model        : meta-llama/Meta-Llama-3.1-8B-Instruct\n",
            "\n",
            "name         : Answer Relevancy\n",
            "score        : 1.0\n",
            "reason       : The score is 1.00 because the actual output directly addresses the question about the use of a pipe, making it highly relevant.\n",
            "model        : meta-llama/Meta-Llama-3.1-8B-Instruct\n",
            "\n",
            "-----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(log_output)"
      ],
      "metadata": {
        "id": "lQaYwsdSF4oQ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional: Compare with OpenAI gpt-4o"
      ],
      "metadata": {
        "id": "6PfOrRkBWsUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use OpenAI you need an Api key. deepEval defaults to openAI if no model is set.\n",
        "We use explicit wrapper implementation to reduce non-determinism,\n",
        "but OpenAI is still not deterministc."
      ],
      "metadata": {
        "id": "92s6WNhzW6Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK-iVhxB-nPp",
        "outputId": "f6ab1a96-f59d-4161-b615-f01e1be996d1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.43.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_output=\"\"\n",
        "deepeval_custom_model = None # deep_eval defaults to openai\n",
        "r=metrics_run(llm_input, llm_output_concise)\n",
        "print_metrics_data(r)"
      ],
      "metadata": {
        "id": "cPMRaclPVwyt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989,
          "referenced_widgets": [
            "160c648b756c4ab8990cf9b24c6b51b1",
            "638b37b240ae4552a42edc30da96982d"
          ]
        },
        "outputId": "b76b3918-dc9c-495d-ae66-17d063122960"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "160c648b756c4ab8990cf9b24c6b51b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating test cases...\n",
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ Conciseness (GEval) (score: 0.672558390477256, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The Actual Output is slightly longer than the Input and adds some detail about 'sometimes solids' that is not explicitly requested but retains essential information., error: None)\n",
            "  - ✅ Language (GEval) (score: 0.6851780405955414, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The grammar and syntax are correct, but the Actual Output does not match the question format of the Input., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the answer is perfectly relevant and addresses the question directly. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What is a pipe used for ?\n",
            "  - actual output: A pipe is a tubular conduit used to transport fluids or sometimes solids.\n",
            "  - expected output: None\n",
            "  - context: None\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Conciseness (GEval): 100.00% pass rate\n",
            "Language (GEval): 100.00% pass rate\n",
            "Answer Relevancy: 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "🎉 Tests finished ✅! Run \u001b[32m'deepeval login'\u001b[0m to view evaluation results on Confident AI. ‼️ NOTE: You can also run \n",
              "evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🎉 Tests finished ✅! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to view evaluation results on Confident AI. ‼️ NOTE: You can also run \n",
              "evaluations on ALL of deepeval's metrics directly on Confident AI instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input        : What is a pipe used for ?\n",
            "actual_output: A pipe is a tubular conduit used to transport fluids or sometimes solids.\n",
            "\n",
            "name         : Conciseness (GEval)\n",
            "score        : 0.672558390477256\n",
            "reason       : The Actual Output is slightly longer than the Input and adds some detail about 'sometimes solids' that is not explicitly requested but retains essential information.\n",
            "model        : gpt-4o\n",
            "\n",
            "name         : Language (GEval)\n",
            "score        : 0.6851780405955414\n",
            "reason       : The grammar and syntax are correct, but the Actual Output does not match the question format of the Input.\n",
            "model        : gpt-4o\n",
            "\n",
            "name         : Answer Relevancy\n",
            "score        : 1.0\n",
            "reason       : The score is 1.00 because the answer is perfectly relevant and addresses the question directly. Great job!\n",
            "model        : gpt-4o\n",
            "\n",
            "-----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(log_output)"
      ],
      "metadata": {
        "id": "XK68XCsv6Iko"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gWiSofqoHgJP"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}