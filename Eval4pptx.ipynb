{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJCordhose/practical-llm/blob/main/Eval4pptx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval - small LLM as a judge\n"
      ],
      "metadata": {
        "id": "vFF4TwdXQdeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-On\n",
        "\n",
        "Prompting for smaller LLMs is even harder than for the powerful ones. These prompts need to generalize beyond a single example.\n",
        "\n",
        "???????? Tasks:\n",
        "* Add an additional Criteria Rule for one the criteria named above and at it to the test suite.\n",
        "* Alternatively try to improve on one of the existing criteria.\n",
        "* Do you think this approach is reasonable? If not, what would you do differently?"
      ],
      "metadata": {
        "id": "jO93wLSFWwHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic SetUp"
      ],
      "metadata": {
        "id": "HUWl-0x-524k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC0CYqfnOg_j",
        "outputId": "2fdfda7a-c290-4856-8b23-8b85a69f483c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Aug 27 16:06:24 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Important:**\n",
        "Ensure that no GPU memory is allocated yet (in the case of a T4 look for \"0MiB / 15360MiB\").\n",
        "If GPU memory is already allocated use Runtime/Manage Sessions to delete all active sessions."
      ],
      "metadata": {
        "id": "waEtPWk5MjyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "wnUuUSG68wlB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!pip install --upgrade -q transformers accelerate flash_attn torch bitsandbytes\n",
        "!pip install lm-format-enforcer -q\n",
        "!pip install deepeval==1.1.1 -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liz4MUJleWLI",
        "outputId": "72293299-194a-4776-94b2-54a7e42cdbc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### => you may need to restart the session"
      ],
      "metadata": {
        "id": "zdHUEIcDBdw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Configure HuggingFace token as a Colab Secret, use key symbol on the left panel\n",
        "!huggingface-cli login --token {userdata.get('HF_TOKEN')}"
      ],
      "metadata": {
        "id": "ispdJ2ZpmbVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load & quantize Model (yielding: model_id, model, tokenizer)"
      ],
      "metadata": {
        "id": "PL_RxgOy58BG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# kind = 'Lllama_3.1_8B_4bit'\n",
        "kind = 'Lllama_3.1_8B_8bit'\n",
        "# kind = 'Lllama_3.1_8B_16bit'  # too large for T4\n",
        "# kind = 'Phi-3.5-MoE_4bit'\n",
        "# kind = \"Phi-3.5-mini_16bit\"\n",
        "\n",
        "if \"Lllama_3.1_8B\" in kind:\n",
        "  model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "elif \"Phi-3.5-MoE\" in kind:\n",
        "  model_id = \"microsoft/Phi-3.5-MoE-instruct\"\n",
        "else:\n",
        "  model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "\n",
        "print(kind)\n",
        "print(model_id)"
      ],
      "metadata": {
        "id": "xyH5zHti6A-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***note:*** execute in a terminal 'watch -n 0.5 nvidia-smi' to see the GPU usage and when the model is loaded onto it"
      ],
      "metadata": {
        "id": "-7yP74Cl6Omy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "torch_dtype = None\n",
        "quantization_config = None\n",
        "\n",
        "if \"8bit\" in kind:\n",
        "  print(\"Using 8Bit quantization\")\n",
        "  quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "elif \"4bit\" in kind:\n",
        "  print(\"Using 4Bit quantization\")\n",
        "  quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "else:\n",
        "  print(\"Using Full Resolution\")\n",
        "  torch_dtype = torch.bfloat16\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device_map=\"cuda\",\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "-YgmOdKMszpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "NX2pljoSlfbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "def llm_run(messages):\n",
        "  if type(messages) == str:\n",
        "    messages = [{\"role\": \"user\", \"content\": messages}]\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "  terminators = [\n",
        "      tokenizer.eos_token_id,\n",
        "      tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  ]\n",
        "\n",
        "  input_token_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        "  ).to(model.device)\n",
        "\n",
        "  outputs = model.generate(\n",
        "      input_token_ids,\n",
        "      max_new_tokens=512,\n",
        "      eos_token_id=terminators,\n",
        "      pad_token_id=tokenizer.eos_token_id,\n",
        "      do_sample=False\n",
        "  )\n",
        "  output_token_ids = outputs[0][input_token_ids.shape[-1]:]\n",
        "  result = tokenizer.decode(output_token_ids, skip_special_tokens=True)\n",
        "  return result"
      ],
      "metadata": {
        "id": "jlJxNTGs6xM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try out our model:"
      ],
      "metadata": {
        "id": "8b-8riV2Vf_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "llm_run(\"who are you ?\")"
      ],
      "metadata": {
        "id": "nf93aT-JVPol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an English-speaking, competent expert in the field of statutory health insurance. Answer consice, serious and formal.\"},\n",
        "    {\"role\": \"user\", \"content\": f'''\n",
        "    What is the result of the assessment? Is a positive or negative recommendation given? Answer with \"Yes\" or \"No\" and then provide a brief justification for your assessment.\"\n",
        "\n",
        "    # Assessment\n",
        "    With the diagnosis named here, the need for compensation to ensure the basic need is conceivable.\n",
        "    '''}\n",
        "  ]\n",
        "\n",
        "answer = llm_run(messages)\n",
        "Markdown(answer)"
      ],
      "metadata": {
        "id": "hGxdDNNY6e8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple, bare-bones Example of an LLM-as-a-judge"
      ],
      "metadata": {
        "id": "z0jYXr3F9pem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_text=\"Witing texts is painful, caus im making mitakes.\"\n",
        "prompt = f'''\n",
        "You are an expert on english language, grading a students text with scores between 0 and 10.\n",
        "A text written in proper english, in a fluent style, containing no grammatical or syntax errors is graded 10.\n",
        "A text written in a different language or with spelling errors gets a low score.\n",
        "Also give a detailed explanation why the score was chosen.\n",
        "Do not repeat the students text in your explanation.\n",
        "\n",
        "Always answer in the following json format:\n",
        "{{\n",
        "    \"score\": 8,\n",
        "    \"reason\": \"some reason\"\n",
        "}}\n",
        "\n",
        "Examples\n",
        "1. Student Text: The Russian-born founder of Telegram, Pavel Durov, is due to appear in a French court in the coming days after his arrest at a Paris airport over alleged offences related to the messaging app.\n",
        "   Answer:\n",
        "   {{\n",
        "    \"score\": 8,\n",
        "    \"reason\": \"The text is written in english and does not contain any syntactical or grammatical erros\"\n",
        "  }}\n",
        "2. Student Text: Sonntagmorgen um kurz vor acht. Es regnet, ein Hauch von Herbst liegt über Zürich.\n",
        "   Answer:\n",
        "   {{\n",
        "    \"score\": 2,\n",
        "    \"reason\": \"The text is written in german and not in english.\"\n",
        "  }}\n",
        "\n",
        "Student Text: {student_text}\n",
        "Answer:\n",
        "'''"
      ],
      "metadata": {
        "id": "1fUQwWQd8W2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "answer=llm_run(prompt)\n",
        "\n",
        "json.loads(answer)"
      ],
      "metadata": {
        "id": "90PhBKcH8Wu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llm-as-a-judge: GEval with deepeval"
      ],
      "metadata": {
        "id": "xtY98fXd6gl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "see: https://docs.confident-ai.com/docs/guides-using-custom-llms\n"
      ],
      "metadata": {
        "id": "Z9fEWGsY6v18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.models import DeepEvalBaseLLM\n",
        "\n",
        "llm_log = True\n",
        "\n",
        "def log(*args, **kwargs):\n",
        "    if llm_log:\n",
        "        print(*args, **kwargs)\n",
        "\n",
        "# wrapper calling llm_run using the global members model_id, model\n",
        "class CustomDeepEvalLlm(DeepEvalBaseLLM):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def load_model(self):\n",
        "        return model\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        log('********************** deepEval LLM Generate BEGIN ********************************************')\n",
        "        log(\"***** Prompt         :\", prompt)\n",
        "        result = llm_run(prompt)\n",
        "        log(\"***** Answer         :\", result)\n",
        "        log('********************** deepEval LLM Generate END   ')\n",
        "        log('')\n",
        "        return result\n",
        "\n",
        "    async def a_generate(self, prompt: str) -> str:\n",
        "        return self.generate(prompt)\n",
        "\n",
        "    def get_model_name(self):\n",
        "        return model_id"
      ],
      "metadata": {
        "id": "pRgPt2He6fEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "try out the wrapper"
      ],
      "metadata": {
        "id": "mdaSg0H8WDp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = CustomDeepEvalLlm()\n",
        "c.generate('who are you ? answer in JSON')"
      ],
      "metadata": {
        "id": "6f4JbIRxUXXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the LLM the evaluate if a generated output was written in proper english.\n",
        "Yielding a score between 0 and 1 as well as a reason, why the score was chosen."
      ],
      "metadata": {
        "id": "LMDtYbSw99Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import deepeval\n",
        "import deepeval.metrics\n",
        "import deepeval.test_case\n",
        "from deepeval.models.base_model import DeepEvalBaseLLM\n",
        "from deepeval.metrics import GEval\n",
        "from deepeval.test_case import LLMTestCaseParams\n",
        "\n",
        "def geval(name, criteria, answer):\n",
        "    deepeval_model = CustomDeepEvalLlm()\n",
        "    test_case = deepeval.test_case.LLMTestCase(\n",
        "        input='once upon a time there was a question',\n",
        "        actual_output=answer\n",
        "      )\n",
        "\n",
        "    conciseness_metric = GEval(\n",
        "        name=name,\n",
        "        criteria=criteria,\n",
        "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
        "        model=deepeval_model,\n",
        "        verbose_mode=False\n",
        "    )\n",
        "\n",
        "    deepeval_metrics = [conciseness_metric]\n",
        "\n",
        "    eval_result = deepeval.evaluate(\n",
        "        test_cases=[test_case],\n",
        "        metrics=deepeval_metrics\n",
        "    )\n",
        "    return eval_result"
      ],
      "metadata": {
        "id": "2x3hftSQaea-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Geval Step 1 Example"
      ],
      "metadata": {
        "id": "p4swZQuDG5GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criteria=\"Determine how concise the actual output is\"\n",
        "geval_step1_prompt = f'''Given an evaluation criteria which outlines how you should judge the Actual Output, generate\n",
        "3-4 concise evaluation steps based on the criteria below. You MUST make it clear how to evaluate Actual Output in\n",
        "relation to one another.\n",
        "\n",
        "Evaluation Criteria:\n",
        "{criteria}\n",
        "\n",
        "**\n",
        "IMPORTANT: Please make sure to only return in JSON format, with the \"steps\" key as a list of strings. No words or\n",
        "explanation is needed.\n",
        "Example JSON:\n",
        "{{\n",
        "    \"steps\": <list_of_strings>\n",
        "}}\n",
        "**\n",
        "\n",
        "JSON:\n",
        "'''\n",
        "answer = llm_run(geval_step1_prompt)\n",
        "\n",
        "json.loads(answer)"
      ],
      "metadata": {
        "id": "fo-nE5KcAVXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Geval Step 2 Example"
      ],
      "metadata": {
        "id": "WuObfnA4HNF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actual_output = \"Pipes are beautiful, black and round. Because they are round they are very convenient and don't have any edges.\"\n",
        "geval_step2_prompt = f'''\n",
        "'''\n",
        "answer = llm_run(geval_step2_prompt)\n",
        "\n",
        "json.loads(answer)"
      ],
      "metadata": {
        "id": "QjYjN-9oH6VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Geval"
      ],
      "metadata": {
        "id": "cu3nSLWzG_JG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_log=False\n",
        "geval(\"Conciseness\", \"Determine how concise the actual output is\", \"Pipes are beautiful.\" )"
      ],
      "metadata": {
        "id": "s-eQQ9sYcCKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TOVD-R8kcX6T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}